# python_agent.py
from langchain.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import ast
import subprocess
import sys
import re
import os
from typing import Optional, List, Dict, Set
import logging
from itertools import chain
import base64
import hashlib
from pathlib import Path

CODE_FENCE_BLOCK = re.compile(r'```(?:python)?\s*([\s\S]+?)\s*```', re.IGNORECASE)
BUILTIN_SYMBOLS = set(dir(__builtins__)) | {"self", "cls"}

def _extract_code_snippet(code: str) -> str:
    """Normalize incoming code by stripping Markdown fences and whitespace."""
    if not code:
        return ""

    stripped = code.strip()
    fence_match = CODE_FENCE_BLOCK.search(stripped)
    if fence_match:
        return fence_match.group(1).strip()

    return stripped

def _safe_unparse(node: Optional[ast.AST]) -> str:
    """Safely convert AST nodes back to source-like text."""
    if node is None:
        return ""
    try:
        return ast.unparse(node)  # type: ignore[attr-defined]
    except Exception:
        if isinstance(node, ast.Name):
            return node.id
        if isinstance(node, ast.Attribute):
            return f"{_safe_unparse(node.value)}.{node.attr}"
        if isinstance(node, ast.Constant):
            return repr(node.value)
        return node.__class__.__name__

def _format_error_context(lines: List[str], lineno: Optional[int], col: Optional[int]) -> str:
    """Generate a short excerpt around a syntax error with a caret pointer."""
    if not lineno or lineno < 1 or lineno > len(lines):
        return ""

    start = max(0, lineno - 2)
    end = min(len(lines), lineno + 1)
    excerpt = []
    for idx in range(start, end):
        marker = ">" if (idx + 1) == lineno else " "
        prefix = f"{marker} ç¬¬{idx + 1}è¡Œ: "
        line_content = lines[idx].replace('\t', '    ')
        excerpt.append(f"{prefix}{line_content}")
        if (idx + 1) == lineno and col and col > 0:
            caret_padding = " " * (len(prefix) + col - 1)
            excerpt.append(f"{caret_padding}^")

    return "\n".join(excerpt)

def _collect_code_structure(tree: ast.AST) -> Dict:
    """Gather high-level information such as imports, classes, and functions."""
    structure = {
        "imports": [],
        "functions": [],
        "classes": [],
        "async_functions": [],
        "type_hint_total": 0,
        "type_hint_annotated": 0
    }

    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                alias_repr = alias.name if not alias.asname else f"{alias.name} as {alias.asname}"
                structure["imports"].append(alias_repr)
        elif isinstance(node, ast.ImportFrom):
            module = node.module or ""
            for alias in node.names:
                alias_repr = alias.name if not alias.asname else f"{alias.name} as {alias.asname}"
                structure["imports"].append(f"from {module} import {alias_repr}")
        elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            args = []

            def _register_arg(arg_node: ast.arg, prefix: str = ""):
                structure["type_hint_total"] += 1
                annotated = bool(arg_node.annotation)
                if annotated:
                    structure["type_hint_annotated"] += 1
                annotation = f": {_safe_unparse(arg_node.annotation)}" if annotated else ""
                args.append(f"{prefix}{arg_node.arg}{annotation}")

            for arg in chain(node.args.posonlyargs, node.args.args):
                _register_arg(arg)

            if node.args.vararg:
                _register_arg(node.args.vararg, prefix="*")

            if node.args.kwonlyargs:
                if not node.args.vararg:
                    args.append("*")
                for arg in node.args.kwonlyargs:
                    _register_arg(arg)

            if node.args.kwarg:
                _register_arg(node.args.kwarg, prefix="**")

            returns = _safe_unparse(getattr(node, 'returns', None))
            fn_info = {
                "name": node.name,
                "args": args,
                "returns": returns,
                "lineno": node.lineno,
                "end_lineno": getattr(node, 'end_lineno', node.lineno),
                "is_async": isinstance(node, ast.AsyncFunctionDef)
            }
            if isinstance(node, ast.AsyncFunctionDef):
                structure["async_functions"].append(fn_info)
            else:
                structure["functions"].append(fn_info)
        elif isinstance(node, ast.ClassDef):
            bases = [_safe_unparse(base) for base in node.bases] or ["object"]
            methods = [n.name for n in node.body if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))]
            structure["classes"].append({
                "name": node.name,
                "bases": bases,
                "methods": methods,
                "lineno": node.lineno,
                "end_lineno": getattr(node, 'end_lineno', node.lineno)
            })

    return structure

def _docstring_gaps(tree: ast.AST) -> List[str]:
    """Identify missing docstrings at module/class/function level."""
    issues = []
    if ast.get_docstring(tree) is None:
        issues.append("æ¨¡å—ç¼ºå°‘é¡¶å±‚ docstring")

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            if ast.get_docstring(node) is None:
                issues.append(f"å‡½æ•° `{node.name}` ç¼ºå°‘ docstring")
        elif isinstance(node, ast.ClassDef):
            if ast.get_docstring(node) is None:
                issues.append(f"ç±» `{node.name}` ç¼ºå°‘ docstring")

    return issues

def _basic_style_checks(lines: List[str]) -> List[str]:
    """Perform lightweight style linting."""
    issues = []
    for idx, line in enumerate(lines, 1):
        raw_line = line.rstrip("\n")
        if len(raw_line) > 100:
            issues.append(f"ç¬¬{idx}è¡Œé•¿åº¦ä¸º{len(raw_line)}å­—ç¬¦ï¼Œå»ºè®®ä¸è¶…è¿‡100å­—ç¬¦")
        if raw_line.rstrip() != raw_line:
            issues.append(f"ç¬¬{idx}è¡Œå­˜åœ¨å°¾éšç©ºæ ¼")
        if raw_line.startswith('\t'):
            issues.append(f"ç¬¬{idx}è¡Œä½¿ç”¨äº†åˆ¶è¡¨ç¬¦ç¼©è¿›ï¼Œå»ºè®®ä½¿ç”¨4ä¸ªç©ºæ ¼")
        leading = len(raw_line) - len(raw_line.lstrip(' '))
        if raw_line and (raw_line[0] == ' ' or raw_line[0] == '\t'):
            if raw_line.startswith(' ') and (leading % 4) != 0:
                issues.append(f"ç¬¬{idx}è¡Œç¼©è¿›ä¸æ˜¯4çš„å€æ•°")

    return issues

def _cyclomatic_complexity(node: ast.AST) -> int:
    """Compute a rough cyclomatic complexity for a function node."""
    complexity = 1
    decision_points = (
        ast.If, ast.For, ast.AsyncFor, ast.While, ast.Try, ast.With, ast.AsyncWith,
        ast.BoolOp, ast.IfExp, ast.Compare, ast.comprehension, ast.ExceptHandler
    )
    for child in ast.walk(node):
        if isinstance(child, decision_points):
            complexity += 1
    return complexity

def _max_nesting_depth(node: ast.AST, depth: int = 0) -> int:
    """Approximate nesting depth for control-flow statements."""
    control_nodes = (ast.If, ast.For, ast.AsyncFor, ast.While, ast.With, ast.AsyncWith, ast.Try)
    max_depth = depth
    for child in ast.iter_child_nodes(node):
        child_depth = depth + 1 if isinstance(child, control_nodes) else depth
        max_depth = max(max_depth, _max_nesting_depth(child, child_depth))
    return max_depth

def _extract_target_names(target: ast.AST) -> Set[str]:
    """Collect variable names introduced by assignment targets."""
    names: Set[str] = set()
    if isinstance(target, ast.Name):
        names.add(target.id)
    elif isinstance(target, (ast.Tuple, ast.List)):
        for elt in target.elts:
            names.update(_extract_target_names(elt))
    elif isinstance(target, ast.Starred):
        names.update(_extract_target_names(target.value))
    return names

def _collect_defined_names(tree: ast.AST) -> Set[str]:
    """Collect names that are defined within the AST (assignments, defs, imports, etc.)."""
    defined: Set[str] = set(BUILTIN_SYMBOLS)

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            defined.add(node.name)

            def _register_args(args: ast.arguments):
                for arg in chain(args.posonlyargs, args.args, args.kwonlyargs):
                    defined.add(arg.arg)
                if args.vararg:
                    defined.add(args.vararg.arg)
                if args.kwarg:
                    defined.add(args.kwarg.arg)

            _register_args(node.args)
        elif isinstance(node, ast.Lambda):
            for arg in node.args.args + node.args.kwonlyargs:
                defined.add(arg.arg)
            if node.args.vararg:
                defined.add(node.args.vararg.arg)
            if node.args.kwarg:
                defined.add(node.args.kwarg.arg)
        elif isinstance(node, ast.ClassDef):
            defined.add(node.name)
        elif isinstance(node, ast.Assign):
            for target in node.targets:
                defined.update(_extract_target_names(target))
        elif isinstance(node, ast.AnnAssign):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.AugAssign):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, (ast.For, ast.AsyncFor)):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.comprehension):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.With):
            for item in node.items:
                if item.optional_vars:
                    defined.update(_extract_target_names(item.optional_vars))
        elif isinstance(node, ast.AsyncWith):
            for item in node.items:
                if item.optional_vars:
                    defined.update(_extract_target_names(item.optional_vars))
        elif isinstance(node, ast.NamedExpr):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.ExceptHandler):
            if isinstance(node.name, str):
                defined.add(node.name)
        elif isinstance(node, ast.Import):
            for alias in node.names:
                defined.add(alias.asname or alias.name.split('.')[0])
        elif isinstance(node, ast.ImportFrom):
            for alias in node.names:
                if alias.name == '*':
                    continue
                defined.add(alias.asname or alias.name)

    return defined

def _detect_name_issues(tree: ast.AST) -> Dict[str, List[int]]:
    """Find names that are referenced but never defined/imported."""
    defined = _collect_defined_names(tree)
    issues: Dict[str, set] = {}

    for node in ast.walk(tree):
        if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):
            if node.id not in defined:
                issues.setdefault(node.id, set()).add(getattr(node, "lineno", 0) or 0)

    return {name: sorted(lines) for name, lines in issues.items() if lines}

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

# Markdownå¤„ç†å™¨ç±»
class MarkdownHandbook:
    """Markdownæ–‡æ¡£å¤„ç†å™¨ï¼Œæ”¯æŒæœç´¢Python-100-Daysæ–‡ä»¶å¤¹ä¸­çš„Markdownæ–‡ä»¶"""
    
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.md_files = []  # æ‰€æœ‰Markdownæ–‡ä»¶è·¯å¾„
        self.content_index = {}  # å…³é”®è¯åˆ°ä½ç½®çš„ç´¢å¼•
        self.image_index = {}    # å›¾ç‰‡åˆ°å†…å®¹çš„æ˜ å°„
        self.sections = {}       # æ–‡ä»¶ç« èŠ‚ç»“æ„
        self.text_cache = {}     # æ–‡ä»¶æ–‡æœ¬ç¼“å­˜
        self.images_cache = {}   # å›¾ç‰‡ç¼“å­˜ï¼ˆæœ¬åœ°å›¾ç‰‡ï¼‰
        self.load_markdown_files()
        
    def load_markdown_files(self):
        """åŠ è½½æ‰€æœ‰Markdownæ–‡ä»¶"""
        try:
            # éå†æ–‡ä»¶å¤¹ï¼Œæ‰¾åˆ°æ‰€æœ‰.mdæ–‡ä»¶
            for md_file in self.base_path.rglob("*.md"):
                self.md_files.append(md_file)
                self._index_file(md_file)
            
            logger.info(f"åŠ è½½äº† {len(self.md_files)} ä¸ªMarkdownæ–‡ä»¶")
            self._build_global_index()
            
        except Exception as e:
            logger.error(f"åŠ è½½Markdownæ–‡ä»¶å¤±è´¥: {e}")
    
    def _index_file(self, md_path: Path):
        """ç´¢å¼•å•ä¸ªMarkdownæ–‡ä»¶"""
        try:
            # è¯»å–Markdownæ–‡ä»¶å†…å®¹
            content = md_path.read_text(encoding='utf-8', errors='ignore')
            
            # ç›¸å¯¹è·¯å¾„ä½œä¸ºé”®
            rel_path = str(md_path.relative_to(self.base_path))
            file_key = rel_path.replace('\\', '/')
            
            # ç¼“å­˜æ–‡æœ¬å†…å®¹
            self.text_cache[file_key] = content
            
            # æå–ç« èŠ‚ç»“æ„
            self._parse_sections(file_key, content)
            
            # æå–å›¾ç‰‡ä¿¡æ¯
            self._extract_images(file_key, content, md_path)
            
            # æå–å…³é”®è¯
            keywords = self._extract_keywords(content)
            for keyword in keywords:
                if keyword not in self.content_index:
                    self.content_index[keyword] = []
                self.content_index[keyword].append({
                    'file': file_key,
                    'content': content[:500],  # æˆªå–å‰500å­—ç¬¦
                    'relevance': 'high'
                })
                
        except Exception as e:
            logger.error(f"ç´¢å¼•æ–‡ä»¶ {md_path} å¤±è´¥: {e}")
    
    def _parse_sections(self, file_key: str, content: str):
        """è§£æMarkdownæ–‡ä»¶çš„ç« èŠ‚ç»“æ„"""
        lines = content.split('\n')
        current_section = "ç®€ä»‹"
        section_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æµ‹Markdownæ ‡é¢˜ï¼ˆ# åˆ° ######ï¼‰
            if line.startswith('#'):
                # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚
                if section_lines:
                    section_key = f"{file_key}#{current_section}"
                    self.sections[section_key] = '\n'.join(section_lines)
                
                # æå–æ–°ç« èŠ‚æ ‡é¢˜
                # ç§»é™¤#å·å’Œç©ºæ ¼
                current_section = line.lstrip('#').strip()
                section_lines = []
            else:
                section_lines.append(line)
        
        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if section_lines:
            section_key = f"{file_key}#{current_section}"
            self.sections[section_key] = '\n'.join(section_lines)
    
    def _extract_images(self, file_key: str, content: str, md_path: Path):
        """æå–Markdownä¸­çš„å›¾ç‰‡ä¿¡æ¯"""
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…Markdownå›¾ç‰‡è¯­æ³•
            # ![alt text](image_url "title")
            img_pattern = r'!\[(.*?)\]\((.*?)(?:\s+"(.*?)")?\)'
            
            for match in re.finditer(img_pattern, content, re.IGNORECASE):
                alt_text = match.group(1) or "å›¾ç‰‡"
                img_url = match.group(2)
                title = match.group(3) or alt_text
                
                # å¤„ç†å›¾ç‰‡è·¯å¾„
                if img_url.startswith('http'):
                    # ç½‘ç»œå›¾ç‰‡ï¼Œç›´æ¥ä½¿ç”¨URL
                    img_key = f"web_{hashlib.md5(img_url.encode()).hexdigest()[:8]}"
                    self.images_cache[img_key] = {
                        'type': 'web',
                        'url': img_url,
                        'alt': alt_text,
                        'title': title,
                        'file': file_key,
                        'base64': None  # ç½‘ç»œå›¾ç‰‡ä¸è½¬æ¢ä¸ºbase64
                    }
                else:
                    # æœ¬åœ°å›¾ç‰‡ï¼Œéœ€è¦å¤„ç†ç›¸å¯¹è·¯å¾„
                    img_path = self._resolve_image_path(img_url, md_path)
                    if img_path and img_path.exists():
                        # è½¬æ¢ä¸ºbase64
                        try:
                            img_bytes = img_path.read_bytes()
                            img_base64 = base64.b64encode(img_bytes).decode('utf-8')
                            
                            # ç¡®å®šMIMEç±»å‹
                            ext = img_path.suffix.lower()
                            mime_types = {
                                '.png': 'image/png',
                                '.jpg': 'image/jpeg',
                                '.jpeg': 'image/jpeg',
                                '.gif': 'image/gif',
                                '.bmp': 'image/bmp',
                                '.webp': 'image/webp'
                            }
                            mime_type = mime_types.get(ext, 'image/jpeg')
                            
                            img_key = f"local_{hashlib.md5(img_path.read_bytes()).hexdigest()[:8]}"
                            self.images_cache[img_key] = {
                                'type': 'local',
                                'path': str(img_path),
                                'url': img_url,
                                'alt': alt_text,
                                'title': title,
                                'file': file_key,
                                'base64': f'data:{mime_type};base64,{img_base64}'
                            }
                            
                            # å»ºç«‹å›¾ç‰‡ç´¢å¼•
                            keywords = self._extract_keywords(alt_text + ' ' + title)
                            for keyword in keywords:
                                if keyword not in self.image_index:
                                    self.image_index[keyword] = []
                                self.image_index[keyword].append(img_key)
                                
                        except Exception as e:
                            logger.warning(f"æ— æ³•è¯»å–å›¾ç‰‡ {img_path}: {e}")
                    
        except Exception as e:
            logger.error(f"æå–å›¾ç‰‡å¤±è´¥: {e}")
    
    def _resolve_image_path(self, img_url: str, md_path: Path):
        """è§£æå›¾ç‰‡ç›¸å¯¹è·¯å¾„"""
        try:
            # è§£ç URLç¼–ç 
            import urllib.parse
            img_url = urllib.parse.unquote(img_url)
            
            # ç§»é™¤å¯èƒ½çš„æŸ¥è¯¢å‚æ•°
            img_url = img_url.split('?')[0]
            
            # å¤„ç†ä¸åŒçš„è·¯å¾„æ ¼å¼
            if img_url.startswith('/'):
                # ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
                return self.base_path / img_url.lstrip('/')
            elif img_url.startswith('./'):
                # ç›¸å¯¹äºå½“å‰æ–‡ä»¶
                return md_path.parent / img_url[2:]
            elif img_url.startswith('../'):
                # ç›¸å¯¹äºä¸Šçº§ç›®å½•
                return md_path.parent / img_url
            else:
                # å‡è®¾ç›¸å¯¹äºå½“å‰æ–‡ä»¶
                return md_path.parent / img_url
                
        except Exception as e:
            logger.error(f"è§£æå›¾ç‰‡è·¯å¾„å¤±è´¥ {img_url}: {e}")
            return None
    
    def _extract_keywords(self, text: str, max_keywords: int = 10):
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        # ç§»é™¤åœç”¨è¯
        stop_words = {'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'æœ‰', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'æˆ–', 'ç­‰'}
        
        # æå–ä¸­è‹±æ–‡å•è¯
        words = re.findall(r'[\u4e00-\u9fff]{2,5}', text) + re.findall(r'\b[a-zA-Z]{3,}\b', text)
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}
        for word in words:
            if word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:max_keywords]]
    
    def _build_global_index(self):
        """æ„å»ºå…¨å±€ç´¢å¼•"""
        python_keywords = {
            'python', 'è¯­æ³•', 'å‡½æ•°', 'ç±»', 'å¯¹è±¡', 'æ¨¡å—', 'åŒ…', 'å¼‚å¸¸', 'è£…é¥°å™¨',
            'ç”Ÿæˆå™¨', 'è¿­ä»£å™¨', 'åˆ—è¡¨', 'å­—å…¸', 'é›†åˆ', 'å…ƒç»„', 'å­—ç¬¦ä¸²',
            'æ–‡ä»¶', 'è¾“å…¥è¾“å‡º', 'å¤šçº¿ç¨‹', 'å¼‚æ­¥', 'ç½‘ç»œ', 'æ•°æ®åº“',
            'æµ‹è¯•', 'è°ƒè¯•', 'æ€§èƒ½', 'ä¼˜åŒ–', 'ç®—æ³•', 'æ•°æ®ç»“æ„', 'çˆ¬è™«',
            'æ•°æ®åˆ†æ', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'webå¼€å‘', 'gui'
        }
        
        for file_key, content in self.text_cache.items():
            # ä¸ºæ¯ä¸ªå…³é”®è¯å»ºç«‹ç´¢å¼•
            for keyword in python_keywords:
                if keyword.lower() in content.lower():
                    if keyword not in self.content_index:
                        self.content_index[keyword] = []
                    
                    # æå–ä¸Šä¸‹æ–‡
                    context = self._get_context(content, keyword, 200)
                    self.content_index[keyword].append({
                        'file': file_key,
                        'context': context,
                        'type': 'keyword_match'
                    })
    
    def _get_context(self, text: str, keyword: str, context_size: int = 200):
        """è·å–å…³é”®è¯ä¸Šä¸‹æ–‡"""
        # ä¸åŒºåˆ†å¤§å°å†™æœç´¢
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        match = pattern.search(text)
        if not match:
            return ""
        
        pos = match.start()
        start = max(0, pos - context_size // 2)
        end = min(len(text), pos + len(keyword) + context_size // 2)
        return text[start:end]
    
    def search_with_images(self, query: str, max_results: int = 5):
        """æœç´¢å†…å®¹å¹¶è¿”å›ç›¸å…³å›¾ç‰‡"""
        results = {
            'text_results': [],
            'image_results': [],
            'sections': []
        }
        
        query_lower = query.lower()
        
        # 1. æœç´¢ç« èŠ‚
        for section_key, content in self.sections.items():
            if query_lower in section_key.lower() or query_lower in content.lower():
                # æå–æ–‡ä»¶åå’Œç« èŠ‚å
                if '#' in section_key:
                    file_part, section_part = section_key.split('#', 1)
                    # æå–ç›¸å…³æ®µè½
                    paragraphs = content.split('\n')
                    relevant_content = []
                    
                    for para in paragraphs:
                        if query_lower in para.lower():
                            clean_para = re.sub(r'\s+', ' ', para).strip()
                            if len(clean_para) > 30:
                                relevant_content.append(clean_para)
                    
                    if relevant_content:
                        results['sections'].append({
                            'file': file_part,
                            'title': section_part,
                            'content': ' '.join(relevant_content[:2]),
                            'full_content': content[:1000]
                        })
        
        # 2. æœç´¢å…³é”®è¯ç´¢å¼•
        for keyword in query_lower.split():
            if keyword in self.content_index:
                for item in self.content_index[keyword][:max_results]:
                    results['text_results'].append({
                        'type': 'keyword',
                        'keyword': keyword,
                        'content': item.get('context', item.get('content', '')),
                        'file': item.get('file', ''),
                        'relevance': item.get('relevance', 'medium')
                    })
        
        # 3. æœç´¢ç›¸å…³å›¾ç‰‡
        for keyword in query_lower.split():
            if keyword in self.image_index:
                for image_key in self.image_index[keyword][:3]:
                    if image_key in self.images_cache:
                        image_info = self.images_cache[image_key]
                        results['image_results'].append({
                            'key': image_key,
                            'caption': image_info['title'],
                            'base64': image_info.get('base64'),
                            'url': image_info.get('url'),
                            'file': image_info['file'],
                            'related_keyword': keyword,
                            'type': image_info['type']
                        })
        
        # å¦‚æœæ²¡æœ‰ç›´æ¥ç»“æœï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
        if not results['text_results'] and not results['image_results']:
            results = self._fuzzy_search(query)
        
        return results
    
    def _fuzzy_search(self, query: str):
        """æ¨¡ç³Šæœç´¢"""
        results = {
            'text_results': [],
            'image_results': [],
            'sections': []
        }
        
        # åœ¨æ‰€æœ‰æ–‡æœ¬ä¸­æœç´¢
        for file_key, text in self.text_cache.items():
            if query.lower() in text.lower():
                context = self._get_context(text, query, 300)
                results['text_results'].append({
                    'type': 'full_text',
                    'content': context,
                    'file': file_key,
                    'relevance': 'medium'
                })
        
        return results
    
    def get_relevant_images(self, topic: str, limit: int = 3):
        """è·å–ç‰¹å®šä¸»é¢˜çš„ç›¸å…³å›¾ç‰‡"""
        images = []
        
        # ä»å›¾ç‰‡ç´¢å¼•ä¸­æŸ¥æ‰¾
        for keyword in topic.lower().split():
            if keyword in self.image_index:
                for image_key in self.image_index[keyword][:limit]:
                    if image_key in self.images_cache:
                        images.append(self.images_cache[image_key])
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›READMEæ–‡ä»¶ä¸­çš„å›¾ç‰‡
        if not images:
            for key, img in self.images_cache.items():
                if 'README' in img['file']:
                    images.append(img)
                    if len(images) >= limit:
                        break
        
        return images
    
    def get_page_images(self, page_num: int):
        """è·å–æŒ‡å®šé¡µé¢çš„æ‰€æœ‰å›¾ç‰‡ï¼ˆä¸ºå…¼å®¹æ€§ä¿ç•™ï¼‰"""
        images = []
        return images
    
    def search_exact_content(self, exact_phrase: str):
        """ç²¾ç¡®çŸ­è¯­æœç´¢"""
        results = []
        
        for file_key, text in self.text_cache.items():
            positions = [m.start() for m in re.finditer(re.escape(exact_phrase), text, re.IGNORECASE)]
            
            for pos in positions[:3]:  # æœ€å¤š3ä¸ªåŒ¹é…
                start = max(0, pos - 100)
                end = min(len(text), pos + len(exact_phrase) + 100)
                context = text[start:end]
                
                results.append({
                    'file': file_key,
                    'position': pos,
                    'context': context,
                    'exact_match': exact_phrase
                })
        
        return results
    
    def generate_citation(self, content: str, max_length: int = 500):
        """ç”Ÿæˆå¼•ç”¨æ ¼å¼çš„å†…å®¹"""
        if not content:
            return ""
        
        # åœ¨æ‰€æœ‰æ–‡ä»¶ä¸­æŸ¥æ‰¾
        for file_key, text in self.text_cache.items():
            if content[:100] in text:
                start_pos = text.find(content[:100])
                if start_pos != -1:
                    return f"ã€ŠPython-100-Daysã€‹{file_key}: {content[:max_length]}..."
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›åŸå§‹å†…å®¹
        return content[:max_length] + "..."
    
    def get_file_content(self, file_path: str):
        """è·å–æŒ‡å®šæ–‡ä»¶çš„å†…å®¹"""
        try:
            full_path = self.base_path / file_path
            if full_path.exists():
                return full_path.read_text(encoding='utf-8', errors='ignore')
        except:
            pass
        return None

class PythonProgrammingAgent:
    def __init__(self):
        self.tools = {
            "code_executor": self.code_executor,
            "syntax_checker": self.syntax_checker,
            "code_analyzer": self.code_analyzer,
            "handbook_search": self.handbook_search,
            "enhanced_handbook_search": self.enhanced_handbook_search
        }

        # åˆå§‹åŒ–Markdownæ‰‹å†Œ
        try:
            base_path = os.path.join(os.path.dirname(__file__), 'static', 'Python-100-Days-master')
            self.enhanced_handbook = MarkdownHandbook(base_path)
            self.handbook = self.enhanced_handbook  # ä¿æŒå‘åå…¼å®¹
            print(f"âœ… Markdownæ‰‹å†ŒåŠ è½½æˆåŠŸ: {len(self.enhanced_handbook.md_files)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            print(f"âŒ Markdownæ‰‹å†Œåˆå§‹åŒ–å¤±è´¥: {e}")
            self.enhanced_handbook = None
            self.handbook = None

        # åˆå§‹åŒ–æ¨¡å‹
        self.llm = None
        try:
            self.llm = ChatOpenAI(
                model="deepseek-chat",
                base_url="https://api.deepseek.com/v1",
                api_key=os.getenv("DEEPSEEK_API_KEY"),
                temperature=0.1
            )
            print("âœ… ä½¿ç”¨ DeepSeek æ¨¡å‹")
        except Exception as e:
            print(f"âŒ DeepSeek åˆå§‹åŒ–å¤±è´¥: {e}")
            try:
                self.llm = ChatOpenAI(
                    model="gpt-3.5-turbo",
                    api_key=os.getenv("OPENAI_API_KEY"),
                    temperature=0.1
                )
                print("âœ… ä½¿ç”¨ OpenAI æ¨¡å‹")
            except Exception as e2:
                print(f"âŒ OpenAI åˆå§‹åŒ–å¤±è´¥: {e2}")
                print("âš ï¸ ä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼ˆæ— APIï¼‰")

        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ï¼Œä¸“é—¨è§£ç­”Pythonç›¸å…³çš„æŠ€æœ¯é—®é¢˜ã€‚ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
1. å‡†ç¡®å›ç­”Pythonè¯­æ³•ã€åº“å‡½æ•°ã€æœ€ä½³å®è·µç­‰é—®é¢˜
2. æä¾›å¯æ‰§è¡Œçš„ä»£ç ç¤ºä¾‹
3. è§£é‡Šä»£ç é€»è¾‘å’ŒåŸç†
4. å¸®åŠ©è°ƒè¯•å’Œä¼˜åŒ–ä»£ç 
5. æä¾›Pythonæœ€æ–°ç‰¹æ€§çš„ä¿¡æ¯
6. å½“å›ç­”æ¶‰åŠã€ŠPython-100-Daysã€‹çš„å†…å®¹æ—¶ï¼Œä¼˜å…ˆå¼•ç”¨æ‰‹å†Œä¸­çš„æƒå¨è§£é‡Š
7. å¦‚æœæ‰‹å†Œä¸­æœ‰ç›¸å…³å›¾è¡¨æˆ–ç¤ºä¾‹å›¾ç‰‡ï¼Œè¯·åœ¨å›ç­”ä¸­æ˜ç¡®æŒ‡å‡ºå¹¶å¼•ç”¨å›¾ç‰‡å†…å®¹
8. å½“ç”¨æˆ·ä¸Šä¼ ä»£ç æˆ–å›¾è¡¨å›¾ç‰‡æ—¶ï¼Œä»”ç»†åˆ†æå›¾ç‰‡å†…å®¹å¹¶ç»™å‡ºä¸“ä¸šå»ºè®®
9. å¯¹äºå¤æ‚æ¦‚å¿µï¼Œç»“åˆæ‰‹å†Œä¸­çš„å›¾ç¤ºè¿›è¡Œè§£é‡Š

è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
- ç¡®ä¿ä»£ç ç¤ºä¾‹æ˜¯æ­£ç¡®ä¸”å¯è¿è¡Œçš„
- è§£é‡Šè¦æ¸…æ™°æ˜“æ‡‚ï¼Œé€‚åˆä¸åŒæ°´å¹³çš„å¼€å‘è€…
- æä¾›å®é™…åº”ç”¨åœºæ™¯
- æŒ‡å‡ºæ½œåœ¨çš„é™·é˜±å’Œæ³¨æ„äº‹é¡¹
- ä¿æŒå›ç­”çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
- ä½¿ç”¨Markdownæ ¼å¼ç¾åŒ–å›ç­”ï¼Œç‰¹åˆ«æ˜¯ä»£ç å—è¦ç”¨```pythonæ ‡è®°
- å¦‚æœä»æ‰‹å†Œä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ³¨æ˜"æ ¹æ®ã€ŠPython-100-Daysã€‹ç¬¬Xé¡µ..."
- å¦‚æœæ‰‹å†Œä¸­æœ‰ç›¸å…³å›¾è¡¨ï¼Œè¯·è¯´æ˜"æ‰‹å†Œä¸­çš„å›¾è¡¨å±•ç¤ºäº†..."
- å¯¹äºå¤æ‚æ¦‚å¿µï¼Œå»ºè®®ç”¨æˆ·æŸ¥çœ‹æ‰‹å†Œä¸­çš„å›¾ç¤º"""

    def enhanced_handbook_search(self, query: str) -> str:
        """å¢å¼ºç‰ˆæ‰‹å†Œæœç´¢ï¼ŒåŒ…å«å›¾ç‰‡"""
        try:
            if self.enhanced_handbook is None:
                return f"ã€ŠPython-100-Daysã€‹æ‰‹å†Œæœªæ­£ç¡®åˆå§‹åŒ–ã€‚"
            
            results = self.enhanced_handbook.search_with_images(query)
            
            if not results['text_results'] and not results['image_results']:
                return f"åœ¨ã€ŠPython-100-Daysã€‹ä¸­æœªæ‰¾åˆ°ä¸'{query}'ç›´æ¥ç›¸å…³çš„å†…å®¹ã€‚"
            
            response = "## ğŸ“š ã€ŠPython-100-Daysã€‹ç›¸å…³å†…å®¹\n\n"
            
            # æ–‡æœ¬å†…å®¹
            if results.get('text_results'):
                response += "### ğŸ“– ç›¸å…³æ–‡æœ¬å†…å®¹\n\n"
                for i, result in enumerate(results['text_results'][:3], 1):
                    citation = self.enhanced_handbook.generate_citation(result['content'])
                    response += f"{i}. **{result.get('file', 'æœªçŸ¥æ–‡ä»¶')}** - {citation}\n\n"
            
            # ç« èŠ‚å†…å®¹
            if results.get('sections'):
                response += "### ğŸ“‘ ç›¸å…³ç« èŠ‚\n\n"
                for i, section in enumerate(results['sections'][:2], 1):
                    response += f"{i}. **{section['title']}** (æ¥è‡ª: {section['file']})\n"
                    response += f"   {section['content'][:200]}...\n\n"
            
            # ç›¸å…³å›¾ç‰‡
            if results.get('image_results'):
                response += "### ğŸ–¼ï¸ ç›¸å…³å›¾è¡¨å’Œç¤ºä¾‹\n\n"
                response += "æ‰‹å†Œä¸­åŒ…å«ä»¥ä¸‹ç›¸å…³å›¾ç¤ºï¼š\n\n"
                for img in results['image_results'][:2]:
                    response += f"- **{img['caption']}** (æ¥è‡ª: {img['file']})\n"
                    
                    # æ ¹æ®å›¾ç‰‡ç±»å‹å¤„ç†
                    if img['type'] == 'local' and img.get('base64'):
                        # æœ¬åœ°å›¾ç‰‡ï¼Œä½¿ç”¨base64
                        response += f"[IMAGE:{img['caption']}]\n{img['base64']}\n[/IMAGE]\n\n"
                    elif img['type'] == 'web' and img.get('url'):
                        # ç½‘ç»œå›¾ç‰‡ï¼Œä½¿ç”¨URL
                        response += f"![{img['caption']}]({img['url']})\n\n"
            
            return response
            
        except Exception as e:
            logger.error(f"å¢å¼ºæ‰‹å†Œæœç´¢å¤±è´¥: {e}")
            return f"æœç´¢æ‰‹å†Œæ—¶å‡ºç°é”™è¯¯: {str(e)}"

    def handbook_search(self, query: str) -> str:
        """ä»Python-100-Daysæ‰‹å†Œä¸­æœç´¢ç›¸å…³ä¿¡æ¯"""
        try:
            # ä½¿ç”¨å¢å¼ºç‰ˆæœç´¢
            return self.enhanced_handbook_search(query)
            
        except Exception as e:
            logger.error(f"æ‰‹å†Œæœç´¢å¤±è´¥: {e}")
            return f"æœç´¢æ‰‹å†Œæ—¶å‡ºç°é”™è¯¯: {str(e)}"

    def code_executor(self, code: str) -> str:
        """æ‰§è¡ŒPythonä»£ç å¹¶è¿”å›ç»“æœã€‚ç”¨äºæµ‹è¯•ä»£ç ç‰‡æ®µæ˜¯å¦æ­£ç¡®è¿è¡Œã€‚"""
        try:
            dangerous_patterns = [
                r'__import__\s*\(', r'eval\s*\(', r'exec\s*\(', r'open\s*\(',
                r'os\.', r'subprocess\.', r'import\s+os', r'import\s+subprocess',
                r'import\s+sys', r'sys\.', r'__builtins__', r'globals\(\)',
                r'locals\(\)', r'rm\s+-', r'del\s+', r'format\s*\(.*\)\.__globals__'
            ]

            for pattern in dangerous_patterns:
                if re.search(pattern, code, re.IGNORECASE):
                    return "é”™è¯¯ï¼šæ£€æµ‹åˆ°å¯èƒ½ä¸å®‰å…¨çš„ä»£ç ï¼Œæ— æ³•æ‰§è¡Œ"

            # é¢å¤–çš„å®‰å…¨æ£€æŸ¥
            if any(keyword in code for keyword in ['__', 'breakpoint', 'compile', 'memoryview']):
                return "é”™è¯¯ï¼šä»£ç åŒ…å«å—é™å…³é”®å­—"

            cleaned_code = self._clean_python_code(code)
            if not cleaned_code:
                return "é”™è¯¯ï¼šä»£ç ä¸ºç©ºæˆ–æ— æ³•å¤„ç†"

            # ä½¿ç”¨subprocessæ‰§è¡Œä»£ç 
            result = subprocess.run(
                [sys.executable, "-c", cleaned_code],
                capture_output=True,
                text=True,
                timeout=10,
                cwd=os.path.dirname(os.path.abspath(__file__)) if __file__ else os.getcwd()
            )

            if result.returncode == 0:
                output = result.stdout.strip()
                return f"âœ… æ‰§è¡ŒæˆåŠŸ:\n{output}" if output else "âœ… ä»£ç æ‰§è¡ŒæˆåŠŸï¼ˆæ— è¾“å‡ºï¼‰"
            else:
                error_msg = result.stderr.strip()
                return f"âŒ æ‰§è¡Œé”™è¯¯:\n{error_msg}"

        except subprocess.TimeoutExpired:
            return "â° é”™è¯¯ï¼šä»£ç æ‰§è¡Œè¶…æ—¶ï¼Œå¯èƒ½å­˜åœ¨æ— é™å¾ªç¯"
        except Exception as e:
            return f"âš ï¸ æ‰§è¡Œå¼‚å¸¸: {str(e)}"

    def syntax_checker(self, code: str) -> str:
        """æ£€æŸ¥Pythonä»£ç çš„è¯­æ³•æ­£ç¡®æ€§ã€‚"""
        cleaned_code = _extract_code_snippet(code)
        if not cleaned_code:
            return "âŒ é”™è¯¯ï¼šä»£ç ä¸ºç©º"

        lines = cleaned_code.splitlines()
        try:
            tree = ast.parse(cleaned_code)
        except SyntaxError as e:
            context = _format_error_context(lines, e.lineno, e.offset)
            error_details = [
                "âŒ è¯­æ³•é”™è¯¯",
                f"- ç±»å‹: {e.msg}",
                f"- ä½ç½®: ç¬¬{e.lineno}è¡Œ, ç¬¬{e.offset}åˆ—" if e.lineno and e.offset else "",
            ]
            if context:
                error_details.append("\nä»£ç ä¸Šä¸‹æ–‡ï¼š")
                error_details.append(context)
            error_details.append("\nå»ºè®®ï¼šæ£€æŸ¥ç¼©è¿›ã€é—æ¼çš„å†’å·ã€æ‹¬å·æˆ–å¼•å·æ˜¯å¦åŒ¹é…ã€‚")
            return "\n".join([line for line in error_details if line])
        except Exception as e:
            return f"âš ï¸ è¯­æ³•æ£€æŸ¥å¼‚å¸¸: {str(e)}"

        structure = _collect_code_structure(tree)
        docstring_issues = _docstring_gaps(tree)
        style_notes = _basic_style_checks(lines)
        name_issues = _detect_name_issues(tree)
        type_total = structure["type_hint_total"] or 1  # é¿å…é™¤0
        type_ratio = structure["type_hint_annotated"] / type_total * 100

        report = [
            "âœ… è¯­æ³•æ£€æŸ¥é€šè¿‡ï¼šæœªå‘ç°è¯­æ³•é”™è¯¯",
            "",
            "ç»“æ„æ¦‚è§ˆ:",
            f"â€¢ æ€»è¡Œæ•°: {len(lines)}",
            f"â€¢ å‡½æ•°æ•°é‡: {len(structure['functions']) + len(structure['async_functions'])}",
            f"â€¢ ç±»æ•°é‡: {len(structure['classes'])}",
            f"â€¢ å¼•å…¥æ¨¡å—: {', '.join(structure['imports']) or 'ï¼ˆæ— ï¼‰'}",
        ]

        if structure["async_functions"]:
            async_names = ", ".join(fn["name"] for fn in structure["async_functions"])
            report.append(f"- å¼‚æ­¥å‡½æ•°: {async_names}")

        report.extend([
            "",
            "ç±»å‹æ³¨è§£è¦†ç›–ç‡:",
            f"â€¢ å‚æ•°æ³¨è§£: {type_ratio:.0f}% ({structure['type_hint_annotated']}/{structure['type_hint_total']})"
            if structure["type_hint_total"]
            else "â€¢ æœªæ£€æµ‹åˆ°å¯ç»Ÿè®¡çš„å‡½æ•°å‚æ•°"
        ])

        if docstring_issues:
            report.extend([
                "",
                "æ–‡æ¡£æç¤º:",
            ])
            for issue in docstring_issues[:5]:
                report.append(f"â€¢ {issue}")
            if len(docstring_issues) > 5:
                report.append("â€¢ ...")

        if name_issues:
            report.extend([
                "",
                "å¯èƒ½çš„åç§°æˆ–ä½œç”¨åŸŸé—®é¢˜:",
            ])
            suggestions = {
                "printf": "æ£€æµ‹åˆ° printfï¼ŒPython ä¸­è¯·ä½¿ç”¨ print()ã€‚",
                "scanf": "æ£€æµ‹åˆ° scanfï¼ŒPython ä¸­å¯ä½¿ç”¨ input()ã€‚",
                "system": "è¯·ç¡®è®¤æ˜¯å¦éœ€è¦ import os åä½¿ç”¨ os.systemã€‚",
            }
            for name in sorted(name_issues.keys())[:6]:
                lines_info = ", ".join(f"ç¬¬{ln}è¡Œ" for ln in name_issues[name][:5])
                extra = suggestions.get(name.lower(), "")
                extra_text = f" {extra}" if extra else ""
                report.append(f"â€¢ {name}: {lines_info}{extra_text}")
            if len(name_issues) > 6:
                report.append("â€¢ ...")

        if style_notes:
            report.extend([
                "",
                "é£æ ¼å»ºè®®ï¼ˆèŠ‚é€‰ï¼‰:",
            ])
            for note in style_notes[:5]:
                report.append(f"â€¢ {note}")
            if len(style_notes) > 5:
                report.append("â€¢ æ›´å¤šé£æ ¼é—®é¢˜è¯·æŸ¥çœ‹å®Œæ•´åˆ†æç»“æœ")

        return "\n".join(report)

    def code_analyzer(self, code: str) -> str:
        """åˆ†æPythonä»£ç ï¼Œæä¾›æ”¹è¿›å»ºè®®å’Œæœ€ä½³å®è·µã€‚"""
        cleaned_code = _extract_code_snippet(code)
        if not cleaned_code:
            return "âŒ é”™è¯¯ï¼šä»£ç ä¸ºç©º"

        lines = cleaned_code.splitlines()
        try:
            tree = ast.parse(cleaned_code)
        except SyntaxError as e:
            context = _format_error_context(lines, e.lineno, e.offset)
            details = [
                "âŒ æ£€æµ‹åˆ°è¯­æ³•é”™è¯¯ï¼Œæ— æ³•ç»§ç»­åˆ†æã€‚",
                f"- é”™è¯¯: {e.msg}",
                f"- ä½ç½®: ç¬¬{e.lineno}è¡Œ, ç¬¬{e.offset}åˆ—" if e.lineno and e.offset else "",
            ]
            if context:
                details.append("\nä»£ç ä¸Šä¸‹æ–‡ï¼š")
                details.append(context)
            details.append("\nè¯·å…ˆä¿®å¤è¯­æ³•é—®é¢˜åå†æ¬¡è¿è¡Œåˆ†æã€‚")
            return "\n".join([line for line in details if line])

        structure = _collect_code_structure(tree)
        docstring_issues = _docstring_gaps(tree)
        style_notes = _basic_style_checks(lines)

        snake_case = re.compile(r'^[a-z_][a-z0-9_]*$')
        pascal_case = re.compile(r'^[A-Z][A-Za-z0-9]+$')

        complexity_notes = []
        maintainability_notes = []
        risk_notes = []
        naming_notes = []

        signature_lookup = {}
        for fn in structure["functions"] + structure["async_functions"]:
            signature_lookup[(fn["name"], fn["lineno"])] = fn

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                signature_meta = signature_lookup.get((node.name, node.lineno))
                args_repr = ", ".join(signature_meta["args"]) if signature_meta else "..."
                signature = f"{'async ' if isinstance(node, ast.AsyncFunctionDef) else ''}{node.name}({args_repr})"
                if signature_meta and signature_meta["returns"]:
                    signature += f" -> {signature_meta['returns']}"

                length = (getattr(node, 'end_lineno', node.lineno) or node.lineno) - node.lineno + 1
                complexity = _cyclomatic_complexity(node)
                depth = _max_nesting_depth(node)

                if complexity >= 12:
                    complexity_notes.append(f"- `{signature}` çš„åœˆå¤æ‚åº¦ä¸º {complexity}ï¼Œå»ºè®®æ‹†è§£é€»è¾‘æˆ–æå–å­å‡½æ•°")
                elif complexity >= 8:
                    complexity_notes.append(f"- `{signature}` çš„åœˆå¤æ‚åº¦ä¸º {complexity}ï¼Œæ¥è¿‘ä¸Šé™ï¼Œæ³¨æ„æ§åˆ¶æ¡ä»¶åˆ†æ”¯")

                if length > 80:
                    maintainability_notes.append(f"- `{signature}` é•¿åº¦ä¸º {length} è¡Œï¼Œå»ºè®®æ‹†åˆ†ä¸ºæ›´å°çš„å‡½æ•°")
                elif length > 40:
                    maintainability_notes.append(f"- `{signature}` é•¿åº¦ä¸º {length} è¡Œï¼Œå¯è€ƒè™‘æå–å…¬å…±é€»è¾‘")

                if depth > 4:
                    maintainability_notes.append(f"- `{signature}` çš„åµŒå¥—æ·±åº¦è¾¾åˆ° {depth} å±‚ï¼Œå»ºè®®é™ä½åµŒå¥—")

                # å¯å˜é»˜è®¤å‚æ•°
                mutable_nodes = (ast.List, ast.Dict, ast.Set)
                defaults = list(node.args.defaults) + [d for d in node.args.kw_defaults if d]
                if any(isinstance(default, mutable_nodes) for default in defaults):
                    risk_notes.append(f"- `{signature}` ä½¿ç”¨å¯å˜å¯¹è±¡ä½œä¸ºé»˜è®¤å‚æ•°ï¼Œå¯èƒ½å¼•å‘å…±äº«çŠ¶æ€é—®é¢˜")

                # å‘½åè§„èŒƒ
                if not snake_case.match(node.name):
                    naming_notes.append(f"- å‡½æ•° `{node.name}` å»ºè®®ä½¿ç”¨ snake_case å‘½å")

            elif isinstance(node, ast.ClassDef):
                if not pascal_case.match(node.name):
                    naming_notes.append(f"- ç±» `{node.name}` å»ºè®®ä½¿ç”¨å¸•æ–¯å¡å‘½åï¼ˆå¦‚ `MyClass`ï¼‰")

            elif isinstance(node, ast.ExceptHandler):
                if node.type is None:
                    risk_notes.append("- æ£€æµ‹åˆ°è£¸ `except:`ï¼Œè¯·æ•è·å…·ä½“å¼‚å¸¸ç±»å‹")
                elif isinstance(node.type, ast.Name) and node.type.id in ("Exception", "BaseException"):
                    risk_notes.append("- æ•è·äº†è¿‡äºå®½æ³›çš„å¼‚å¸¸ `Exception/BaseException`ï¼Œå»ºè®®æ›´ç²¾ç¡®")

            elif isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name) and node.func.id in {"eval", "exec"}:
                    risk_notes.append(f"- ä½¿ç”¨ `{node.func.id}` å¯èƒ½å¸¦æ¥å®‰å…¨é£é™©")
                elif isinstance(node.func, ast.Attribute):
                    attr = node.func.attr
                    owner = getattr(node.func.value, 'id', None)
                    if owner == 'os' and attr in {'system', 'popen'}:
                        risk_notes.append(f"- è°ƒç”¨ `os.{attr}` å¯èƒ½æ‰§è¡Œç³»ç»Ÿå‘½ä»¤ï¼Œè¯·ç¡®è®¤å®‰å…¨æ€§")
                    if owner == 'subprocess' and attr in {'Popen', 'call', 'run'}:
                        risk_notes.append("- ä½¿ç”¨ `subprocess` æ‰§è¡Œå¤–éƒ¨å‘½ä»¤ï¼Œè¯·ç¡®ä¿å‚æ•°å®‰å…¨")

            elif isinstance(node, ast.Global):
                risk_notes.append(f"- åœ¨ç¬¬{node.lineno}è¡Œä½¿ç”¨ `global` ï¼Œå»ºè®®é€šè¿‡å‚æ•°æˆ–è¿”å›å€¼ä¼ é€’æ•°æ®")

        todo_lines = [idx for idx, line in enumerate(lines, 1) if 'TODO' in line or 'FIXME' in line]
        if todo_lines:
            maintainability_notes.append(f"- æ£€æµ‹åˆ°æœªå®Œæˆæ ‡è®°ï¼ˆTODO/FIXMEï¼‰ä½äºè¡Œ: {', '.join(map(str, todo_lines[:5]))}")

        pattern_checks = [
            (r'from\s+\w+\s+import\s+\*', "- é¿å… `from module import *`ï¼Œå¯èƒ½æ±¡æŸ“å‘½åç©ºé—´"),
            (r'while\s+True\s*:', "- `while True` è¯·ç¡®ä¿å­˜åœ¨é€€å‡ºæ¡ä»¶"),
        ]
        for pattern, message in pattern_checks:
            if re.search(pattern, cleaned_code):
                risk_notes.append(message)

        report = ["ä»£ç åˆ†æç»“æœï¼ˆä¸“ä¸šå¢å¼ºç‰ˆï¼‰"]
        report.extend([
            "",
            "ç»“æ„æ¦‚è§ˆ:",
            f"â€¢ è¡Œæ•°: {len(lines)}",
            f"â€¢ å‡½æ•°/å¼‚æ­¥å‡½æ•°: {len(structure['functions']) + len(structure['async_functions'])}",
            f"â€¢ ç±»: {len(structure['classes'])}",
            f"â€¢ å¯¼å…¥: {', '.join(structure['imports']) or 'ï¼ˆæ— ï¼‰'}",
        ])

        if structure["type_hint_total"]:
            ratio = structure["type_hint_annotated"] / structure["type_hint_total"] * 100
            report.append(
                f"â€¢ å‚æ•°ç±»å‹æ³¨è§£è¦†ç›–ç‡: {ratio:.0f}% ({structure['type_hint_annotated']}/{structure['type_hint_total']})")

        if complexity_notes:
            report.extend([
                "",
                "å¤æ‚åº¦ä¸å¯ç»´æŠ¤æ€§:",
                *complexity_notes[:6]
            ])
        if maintainability_notes:
            if "å¤æ‚åº¦ä¸å¯ç»´æŠ¤æ€§:" not in report:
                report.extend(["", "å¤æ‚åº¦ä¸å¯ç»´æŠ¤æ€§:"])
            for note in maintainability_notes[:6]:
                report.append(note)

        if docstring_issues:
            report.extend([
                "",
                "æ–‡æ¡£ä¸å¯è¯»æ€§:",
            ])
            for issue in docstring_issues[:6]:
                report.append(f"â€¢ {issue}")

        if naming_notes:
            report.extend([
                "",
                "å‘½åè§„èŒƒ:",
            ])
            for note in naming_notes[:5]:
                report.append(f"â€¢ {note}")

        if style_notes:
            report.extend([
                "",
                "é£æ ¼å»ºè®®ï¼ˆèŠ‚é€‰ï¼‰:",
            ])
            for note in style_notes[:6]:
                report.append(f"â€¢ {note}")

        if risk_notes:
            report.extend([
                "",
                "æ½œåœ¨é£é™©:",
            ])
            for note in risk_notes[:6]:
                report.append(f"â€¢ {note}")

        if not any([complexity_notes, maintainability_notes, docstring_issues, naming_notes, style_notes, risk_notes]):
            report.extend([
                "",
                "âœ… æœªå‘ç°æ˜æ˜¾çš„é£æ ¼æˆ–è´¨é‡é—®é¢˜ï¼Œä»£ç æ•´ä½“è‰¯å¥½ã€‚"
            ])

        # æ·»åŠ æ‰‹å†Œç›¸å…³å†…å®¹
        try:
            # ä»ä»£ç ä¸­æå–å…³é”®è¯
            keywords = self._extract_code_keywords(cleaned_code)
            if keywords:
                handbook_content = ""
                for keyword in keywords[:2]:  # å–å‰2ä¸ªå…³é”®è¯
                    handbook_result = self.enhanced_handbook_search(keyword)
                    if "æœªæ‰¾åˆ°" not in handbook_result:
                        handbook_content += f"\n\nå…³äº **{keyword}** çš„æ‰‹å†Œå‚è€ƒï¼š\n{handbook_result}"
                
                if handbook_content:
                    report.append("\n" + "="*50)
                    report.append("ğŸ“š ç›¸å…³æ‰‹å†Œå†…å®¹")
                    report.append(handbook_content)
        except Exception as e:
            logger.error(f"æ·»åŠ æ‰‹å†Œå†…å®¹å¤±è´¥: {e}")

        return "\n".join(report)

    def _clean_python_code(self, code: str) -> str:
        """æ¸…ç†è¦æ‰§è¡Œçš„Pythonä»£ç """
        code = re.sub(r'```python\s*|\s*```', '', code).strip()
        if not code:
            return ""

        lines = code.split('\n')
        last_line = lines[-1].strip()

        # å¯¹å­¤ç«‹è¡¨è¾¾å¼æ·»åŠ printï¼ˆä»…åœ¨å®‰å…¨çš„æƒ…å†µä¸‹ï¼‰
        if (last_line and not last_line.startswith(
                (' ', '\t', 'def ', 'class ', 'import ', 'from ', 'if ', 'for ', 'while ', 'with ', '#', 'print(',
                 'return', 'yield', 'raise', 'try', 'except', 'finally'))
                and '=' not in last_line
                and not last_line.endswith((':', ';'))
                and not any(keyword in last_line for keyword in ['lambda', 'async', 'await'])):
            lines[-1] = f"print({last_line})"

        return '\n'.join(lines)

    def _extract_code_keywords(self, code: str) -> List[str]:
        """ä»ä»£ç ä¸­æå–å…³é”®è¯"""
        # å¸¸è§çš„Pythonå…³é”®å­—å’Œåº“
        python_keywords = {
            'def', 'class', 'import', 'from', 'as', 'try', 'except', 'finally',
            'with', 'async', 'await', 'yield', 'lambda', 'global', 'nonlocal'
        }
        
        # å¸¸è§çš„åº“å
        common_libs = {
            'numpy', 'pandas', 'matplotlib', 'requests', 'flask', 'django',
            'tensorflow', 'pytorch', 'sklearn', 'sqlalchemy', 'json', 'csv'
        }
        
        keywords = set()
        
        # æå–å¯¼å…¥çš„åº“
        import_lines = re.findall(r'^(?:import|from)\s+([a-zA-Z0-9_\.]+)', code, re.MULTILINE)
        for lib in import_lines:
            lib_name = lib.split('.')[0]
            if lib_name in common_libs:
                keywords.add(lib_name)
        
        # æå–å‡½æ•°å’Œç±»å
        func_names = re.findall(r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', code)
        class_names = re.findall(r'class\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:\(|:)', code)
        
        keywords.update(func_names)
        keywords.update(class_names)
        
        # æ·»åŠ ä»£ç ä¸­å‡ºç°çš„Pythonå…³é”®å­—
        for keyword in python_keywords:
            if keyword in code:
                keywords.add(keyword)
        
        return list(keywords)

    def _detect_tool_usage(self, question: str) -> dict:
        """æ£€æµ‹ç”¨æˆ·é—®é¢˜æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·"""
        question_lower = question.lower()

        tool_usage = {
            "use_tool": False,
            "tool_name": None,
            "tool_input": None
        }

        # æ£€æµ‹åŸºç¡€æ¦‚å¿µé—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨æ‰‹å†Œæœç´¢
        basic_concepts = ['æ˜¯ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'æ¦‚å¿µ', 'ä»‹ç»', 'è®²è§£', 'è¯´æ˜', 'å«ä¹‰']
        if any(concept in question_lower for concept in basic_concepts):
            # æå–å…³é”®è¯
            words = question_lower.split()
            keywords = [word for word in words if len(word) > 2 and word not in ['python', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€æ ·']]
            if keywords:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "enhanced_handbook_search",
                    "tool_input": ' '.join(keywords[:3])  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
                })

        # æ£€æµ‹ä»£ç æ‰§è¡Œ
        elif any(word in question_lower for word in ['æ‰§è¡Œ', 'è¿è¡Œ', 'è¿è¡Œä»£ç ', 'æ‰§è¡Œä»£ç ', 'test', 'run']):
            code_match = re.search(r'```python\s*(.*?)\s*```', question, re.DOTALL)
            if code_match:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "code_executor",
                    "tool_input": code_match.group(1)
                })

        # æ£€æµ‹è¯­æ³•æ£€æŸ¥
        elif any(word in question_lower for word in ['è¯­æ³•', 'è¯­æ³•æ£€æŸ¥', 'è¯­æ³•é”™è¯¯', 'syntax']):
            code_match = re.search(r'```python\s*(.*?)\s*```', question, re.DOTALL)
            if code_match:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "syntax_checker",
                    "tool_input": code_match.group(1)
                })

        # æ£€æµ‹ä»£ç åˆ†æ
        elif any(word in question_lower for word in ['åˆ†æ', 'ä¼˜åŒ–', 'æ”¹è¿›', 'ä»£ç åˆ†æ', 'analyze']):
            code_match = re.search(r'```python\s*(.*?)\s*```', question, re.DOTALL)
            if code_match:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "code_analyzer",
                    "tool_input": code_match.group(1)
                })

        return tool_usage

    def _should_search_handbook(self, question: str) -> bool:
        """åˆ¤æ–­é—®é¢˜æ˜¯å¦éœ€è¦æœç´¢æ‰‹å†Œ"""
        question_lower = question.lower()
        
        # åŸºç¡€æ¦‚å¿µé—®é¢˜
        basic_concepts = [
            'æ˜¯ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'æ¦‚å¿µ', 'ä»‹ç»', 'è®²è§£', 'è¯´æ˜', 'å«ä¹‰',
            'æ€ä¹ˆç†è§£', 'å¦‚ä½•ç†è§£', 'ä»€ä¹ˆæ„æ€', 'æœ‰ä»€ä¹ˆåŒºåˆ«', 'æœ‰ä»€ä¹ˆä¸åŒ',
            'ä¼˜ç‚¹', 'ç¼ºç‚¹', 'ç‰¹ç‚¹', 'ç‰¹å¾', 'ç‰¹æ€§'
        ]
        
        # å…·ä½“æŠ€æœ¯é—®é¢˜
        technical_terms = [
            'è£…é¥°å™¨', 'ç”Ÿæˆå™¨', 'è¿­ä»£å™¨', 'ä¸Šä¸‹æ–‡ç®¡ç†å™¨', 'å…ƒç±»', 'æè¿°ç¬¦',
            'GIL', 'åƒåœ¾å›æ”¶', 'å†…å­˜ç®¡ç†', 'å¤šçº¿ç¨‹', 'å¤šè¿›ç¨‹', 'åç¨‹',
            'å¼‚æ­¥', 'await', 'async', 'åˆ—è¡¨æ¨å¯¼', 'å­—å…¸æ¨å¯¼', 'é›†åˆæ¨å¯¼',
            'lambda', 'é—­åŒ…', 'ä½œç”¨åŸŸ', 'å‘½åç©ºé—´', 'æ¨¡å—', 'åŒ…'
        ]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŸºç¡€æ¦‚å¿µé—®é¢˜
        for concept in basic_concepts:
            if concept in question_lower:
                return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“æŠ€æœ¯æœ¯è¯­
        for term in technical_terms:
            if term in question_lower:
                return True
        
        return False

    def _get_relevant_handbook_content(self, question: str) -> Optional[str]:
        """è·å–ç›¸å…³çš„æ‰‹å†Œå†…å®¹"""
        try:
            # æå–é—®é¢˜ä¸­çš„å…³é”®è¯
            keywords = re.findall(r'[\u4e00-\u9fff]{2,5}|[a-zA-Z]{3,}', question)
            
            for keyword in keywords:
                if len(keyword) >= 2:  # è‡³å°‘2ä¸ªå­—ç¬¦
                    result = self.enhanced_handbook_search(keyword)
                    if "æœªæ‰¾åˆ°" not in result:
                        return result
            
            return None
        except:
            return None

    def _integrate_handbook_content(self, base_answer: str, handbook_content: str) -> str:
        """å°†æ‰‹å†Œå†…å®¹æ•´åˆåˆ°å›ç­”ä¸­"""
        # ç®€å•çš„æ•´åˆï¼šåœ¨å›ç­”å¼€å¤´æ·»åŠ æ‰‹å†Œå†…å®¹
        integration = f"""
## ğŸ” æ‰‹å†Œå‚è€ƒ

{handbook_content}

---

## ğŸ’¡ æˆ‘çš„è§£ç­”

{base_answer}

> ğŸ“š ä»¥ä¸Šå›ç­”å‚è€ƒäº†ã€ŠPython-100-Daysã€‹ä¸­çš„ç›¸å…³å†…å®¹ï¼Œç¡®ä¿äº†è§£é‡Šçš„å‡†ç¡®æ€§å’Œæƒå¨æ€§ã€‚
"""
        return integration

    def ask_question(self, question: str) -> str:
        """å‘æ™ºèƒ½ä½“æé—®å…³äºPythonç¼–ç¨‹çš„é—®é¢˜"""
        try:
            # æ£€æµ‹æ˜¯å¦ä¸ºéœ€è¦æ‰‹å†Œå¼•ç”¨çš„é—®é¢˜
            should_search_handbook = self._should_search_handbook(question)
            
            # å…ˆè·å–æ‰‹å†Œå†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            handbook_content = None
            if should_search_handbook and self.enhanced_handbook:
                handbook_content = self._get_relevant_handbook_content(question)
            
            # å‡†å¤‡æé—®å†…å®¹
            enhanced_question = question
            if handbook_content:
                # å°†æ‰‹å†Œå†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡æ·»åŠ åˆ°é—®é¢˜ä¸­
                enhanced_question = f"""
ç”¨æˆ·é—®é¢˜: {question}

æ ¹æ®ã€ŠPython-100-Daysã€‹ç›¸å…³å†…å®¹:
{handbook_content}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œç¡®ä¿å›ç­”å‡†ç¡®ä¸”å¼•ç”¨æ‰‹å†Œä¸­çš„æƒå¨è§£é‡Šã€‚
"""
            
            # æ£€æµ‹æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
            tool_info = self._detect_tool_usage(question)

            if tool_info["use_tool"]:
                tool_name = tool_info["tool_name"]
                tool_input = tool_info["tool_input"]

                if tool_name in self.tools:
                    print(f"ğŸ”§ ä½¿ç”¨å·¥å…·: {tool_name}")
                    tool_result = self.tools[tool_name](tool_input)

                    # å¦‚æœæœ‰LLMï¼Œè®©LLMæ¥è§£é‡Šå·¥å…·ç»“æœ
                    if self.llm:
                        enhanced_prompt = f"""ç”¨æˆ·çš„é—®é¢˜: {question}

å·¥å…·æ‰§è¡Œç»“æœ:
{tool_result}

è¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœï¼Œç»™ç”¨æˆ·ä¸€ä¸ªå®Œæ•´ã€ä¸“ä¸šçš„å›ç­”ã€‚ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨Markdownæ ¼å¼ã€‚å¦‚æœå¯èƒ½ï¼Œå¼•ç”¨ã€ŠPython-100-Daysã€‹ä¸­çš„ç›¸å…³å†…å®¹ã€‚"""

                        messages = [
                            SystemMessage(
                                content="ä½ æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹åŠ©æ‰‹ï¼Œè¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœå’Œã€ŠPython-100-Daysã€‹ç»™ç”¨æˆ·æä¾›ä¸“ä¸šã€å®Œæ•´çš„å›ç­”ã€‚"),
                            HumanMessage(content=enhanced_prompt)
                        ]
                        response = self.llm.invoke(messages)
                        return response.content
                    else:
                        # æ— LLMæ—¶ç›´æ¥è¿”å›å·¥å…·ç»“æœ
                        return f"**å·¥å…·æ‰§è¡Œç»“æœ**:\n\n{tool_result}"

            # å¦‚æœæ²¡æœ‰ä½¿ç”¨å·¥å…·æˆ–è€…å·¥å…·ä½¿ç”¨å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨LLM
            if self.llm:
                messages = [
                    SystemMessage(content=self.system_prompt),
                    HumanMessage(content=enhanced_question)
                ]
                response = self.llm.invoke(messages)
                answer = response.content
                
                # å¦‚æœæ‰‹å†Œæœ‰ç›¸å…³å†…å®¹ä¸”æ²¡åŒ…å«åœ¨å›ç­”ä¸­ï¼Œæ·»åŠ å¼•ç”¨
                if handbook_content and "ã€ŠPython-100-Daysã€‹" not in answer:
                    answer = self._integrate_handbook_content(answer, handbook_content)
                
                return answer
            else:
                return self._local_answer(question)

        except Exception as e:
            error_msg = f"æé—®é”™è¯¯: {str(e)}"
            print(f"Error: {error_msg}")
            return f"âš ï¸ {error_msg}\nè¯·æ£€æŸ¥APIå¯†é’¥æˆ–ç½‘ç»œè¿æ¥"

    def _local_answer(self, question: str) -> str:
        """æ— APIæ—¶çš„æœ¬åœ°å›ç­”"""
        q = question.lower()

        # å°è¯•ä½¿ç”¨å·¥å…·
        tool_info = self._detect_tool_usage(question)
        if tool_info["use_tool"] and tool_info["tool_name"] in self.tools:
            return self.tools[tool_info["tool_name"]](tool_info["tool_input"])

        # å°è¯•ä»æ‰‹å†Œä¸­æœç´¢
        try:
            words = q.split()
            keywords = [word for word in words if len(word) > 2]
            if keywords:
                handbook_result = self.enhanced_handbook_search(' '.join(keywords[:2]))
                if "æœªæ‰¾åˆ°" not in handbook_result:
                    return handbook_result
        except:
            pass

        # æœ¬åœ°çŸ¥è¯†åº“
        if any(w in q for w in ['åˆ—è¡¨æ¨å¯¼', 'list comprehension']):
            return "## åˆ—è¡¨æ¨å¯¼å¼\n\nåˆ—è¡¨æ¨å¯¼å¼æä¾›äº†åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ã€‚\n\n**è¯­æ³•**:\n```python\n[expression for item in iterable if condition]\n```\n\n**ç¤ºä¾‹**:\n```python\nsquares = [x**2 for x in range(5)]  # [0, 1, 4, 9, 16]\neven_squares = [x**2 for x in range(10) if x % 2 == 0]  # [0, 4, 16, 36, 64]\npairs = [(x, y) for x in range(3) for y in range(3)]  # åµŒå¥—å¾ªç¯\n```"
        elif any(w in q for w in ['è£…é¥°å™¨', 'decorator']):
            return "## è£…é¥°å™¨\n\nè£…é¥°å™¨ç”¨äºä¿®æ”¹å‡½æ•°/ç±»çš„è¡Œä¸ºã€‚\n\n**è¯­æ³•**:\n```python\n@decorator\ndef function():\n    pass\n```\n\n**ç¤ºä¾‹**:\n```python\nimport time\ndef timer_decorator(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        print(f'è¿è¡Œæ—¶é—´: {time.time()-start:.2f}ç§’')\n        return result\n    return wrapper\n\n@timer_decorator\ndef slow_func():\n    time.sleep(1)\n    return 'å®Œæˆ'\n```"
        elif any(w in q for w in ['ç”Ÿæˆå™¨', 'generator']):
            return "## ç”Ÿæˆå™¨\n\nç”Ÿæˆå™¨æ˜¯èŠ‚çœå†…å­˜çš„è¿­ä»£å™¨ï¼Œä½¿ç”¨yieldå…³é”®å­—ã€‚\n\n**è¯­æ³•**:\n```python\ndef generator_func():\n    yield value\n```\n\n**ç¤ºä¾‹**:\n```python\ndef number_gen(n):\n    for i in range(n):\n        yield i\n\n# ç”Ÿæˆå™¨è¡¨è¾¾å¼\nsquares = (x**2 for x in range(5))\n```"
        elif any(w in q for w in ['è¯­æ³•', 'syntax', 'æ£€æŸ¥']):
            return "è¯·æä¾›Pythonä»£ç ï¼Œæˆ‘å°†æ£€æŸ¥è¯­æ³•æ­£ç¡®æ€§ï¼ˆç¤ºä¾‹ï¼š\n```python\ndef add(a,b): return a+b\n```ï¼‰"
        elif any(w in q for w in ['æ‰§è¡Œ', 'è¿è¡Œ', 'test', 'run']):
            return "è¯·æä¾›Pythonä»£ç ï¼Œæˆ‘å°†æ‰§è¡Œå¹¶è¿”å›ç»“æœï¼ˆç¤ºä¾‹ï¼š\n```python\nprint([i for i in range(5)])\n```ï¼‰"
        elif any(w in q for w in ['åˆ†æ', 'ä¼˜åŒ–', 'æ”¹è¿›']):
            return "è¯·æä¾›Pythonä»£ç ï¼Œæˆ‘å°†åˆ†æå¹¶ç»™å‡ºæ”¹è¿›å»ºè®®ï¼ˆç¤ºä¾‹ï¼š\n```python\ndef calc():\n    total=0\n    for i in range(10): total+=i\n    print(total)\n```ï¼‰"
        else:
            return "âš ï¸ æœ¬åœ°æ¨¡å¼ä»…æ”¯æŒä»¥ä¸‹ä¸»é¢˜ï¼š\n- åˆ—è¡¨æ¨å¯¼å¼ã€è£…é¥°å™¨ã€ç”Ÿæˆå™¨\n- ä»£ç è¯­æ³•æ£€æŸ¥ã€æ‰§è¡Œã€åˆ†æä¼˜åŒ–\nå¦‚éœ€æ›´å¤šå›ç­”ï¼Œè¯·é…ç½®DeepSeek/OpenAI APIå¯†é’¥"
