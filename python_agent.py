# python_agent.py
from langchain.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import json
import ast
import subprocess
import sys
import re
import os
import PyPDF2
from typing import Optional, List, Dict, Set
import logging
from itertools import chain
import base64
import hashlib
from pathlib import Path
import fitz  # PyMuPDF
import numpy as np

CODE_FENCE_BLOCK = re.compile(r'```(?:python)?\s*([\s\S]+?)\s*```', re.IGNORECASE)
BUILTIN_SYMBOLS = set(dir(__builtins__)) | {"self", "cls"}

def _extract_code_snippet(code: str) -> str:
    """Normalize incoming code by stripping Markdown fences and whitespace."""
    if not code:
        return ""

    stripped = code.strip()
    fence_match = CODE_FENCE_BLOCK.search(stripped)
    if fence_match:
        return fence_match.group(1).strip()

    return stripped

def _safe_unparse(node: Optional[ast.AST]) -> str:
    """Safely convert AST nodes back to source-like text."""
    if node is None:
        return ""
    try:
        return ast.unparse(node)  # type: ignore[attr-defined]
    except Exception:
        if isinstance(node, ast.Name):
            return node.id
        if isinstance(node, ast.Attribute):
            return f"{_safe_unparse(node.value)}.{node.attr}"
        if isinstance(node, ast.Constant):
            return repr(node.value)
        return node.__class__.__name__

def _format_error_context(lines: List[str], lineno: Optional[int], col: Optional[int]) -> str:
    """Generate a short excerpt around a syntax error with a caret pointer."""
    if not lineno or lineno < 1 or lineno > len(lines):
        return ""

    start = max(0, lineno - 2)
    end = min(len(lines), lineno + 1)
    excerpt = []
    for idx in range(start, end):
        marker = ">" if (idx + 1) == lineno else " "
        prefix = f"{marker} ç¬¬{idx + 1}è¡Œ: "
        line_content = lines[idx].replace('\t', '    ')
        excerpt.append(f"{prefix}{line_content}")
        if (idx + 1) == lineno and col and col > 0:
            caret_padding = " " * (len(prefix) + col - 1)
            excerpt.append(f"{caret_padding}^")

    return "\n".join(excerpt)

def _collect_code_structure(tree: ast.AST) -> Dict:
    """Gather high-level information such as imports, classes, and functions."""
    structure = {
        "imports": [],
        "functions": [],
        "classes": [],
        "async_functions": [],
        "type_hint_total": 0,
        "type_hint_annotated": 0
    }

    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                alias_repr = alias.name if not alias.asname else f"{alias.name} as {alias.asname}"
                structure["imports"].append(alias_repr)
        elif isinstance(node, ast.ImportFrom):
            module = node.module or ""
            for alias in node.names:
                alias_repr = alias.name if not alias.asname else f"{alias.name} as {alias.asname}"
                structure["imports"].append(f"from {module} import {alias_repr}")
        elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            args = []

            def _register_arg(arg_node: ast.arg, prefix: str = ""):
                structure["type_hint_total"] += 1
                annotated = bool(arg_node.annotation)
                if annotated:
                    structure["type_hint_annotated"] += 1
                annotation = f": {_safe_unparse(arg_node.annotation)}" if annotated else ""
                args.append(f"{prefix}{arg_node.arg}{annotation}")

            for arg in chain(node.args.posonlyargs, node.args.args):
                _register_arg(arg)

            if node.args.vararg:
                _register_arg(node.args.vararg, prefix="*")

            if node.args.kwonlyargs:
                if not node.args.vararg:
                    args.append("*")
                for arg in node.args.kwonlyargs:
                    _register_arg(arg)

            if node.args.kwarg:
                _register_arg(node.args.kwarg, prefix="**")

            returns = _safe_unparse(getattr(node, 'returns', None))
            fn_info = {
                "name": node.name,
                "args": args,
                "returns": returns,
                "lineno": node.lineno,
                "end_lineno": getattr(node, 'end_lineno', node.lineno),
                "is_async": isinstance(node, ast.AsyncFunctionDef)
            }
            if isinstance(node, ast.AsyncFunctionDef):
                structure["async_functions"].append(fn_info)
            else:
                structure["functions"].append(fn_info)
        elif isinstance(node, ast.ClassDef):
            bases = [_safe_unparse(base) for base in node.bases] or ["object"]
            methods = [n.name for n in node.body if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))]
            structure["classes"].append({
                "name": node.name,
                "bases": bases,
                "methods": methods,
                "lineno": node.lineno,
                "end_lineno": getattr(node, 'end_lineno', node.lineno)
            })

    return structure

def _docstring_gaps(tree: ast.AST) -> List[str]:
    """Identify missing docstrings at module/class/function level."""
    issues = []
    if ast.get_docstring(tree) is None:
        issues.append("æ¨¡å—ç¼ºå°‘é¡¶å±‚ docstring")

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            if ast.get_docstring(node) is None:
                issues.append(f"å‡½æ•° `{node.name}` ç¼ºå°‘ docstring")
        elif isinstance(node, ast.ClassDef):
            if ast.get_docstring(node) is None:
                issues.append(f"ç±» `{node.name}` ç¼ºå°‘ docstring")

    return issues

def _basic_style_checks(lines: List[str]) -> List[str]:
    """Perform lightweight style linting."""
    issues = []
    for idx, line in enumerate(lines, 1):
        raw_line = line.rstrip("\n")
        if len(raw_line) > 100:
            issues.append(f"ç¬¬{idx}è¡Œé•¿åº¦ä¸º{len(raw_line)}å­—ç¬¦ï¼Œå»ºè®®ä¸è¶…è¿‡100å­—ç¬¦")
        if raw_line.rstrip() != raw_line:
            issues.append(f"ç¬¬{idx}è¡Œå­˜åœ¨å°¾éšç©ºæ ¼")
        if raw_line.startswith('\t'):
            issues.append(f"ç¬¬{idx}è¡Œä½¿ç”¨äº†åˆ¶è¡¨ç¬¦ç¼©è¿›ï¼Œå»ºè®®ä½¿ç”¨4ä¸ªç©ºæ ¼")
        leading = len(raw_line) - len(raw_line.lstrip(' '))
        if raw_line and (raw_line[0] == ' ' or raw_line[0] == '\t'):
            if raw_line.startswith(' ') and (leading % 4) != 0:
                issues.append(f"ç¬¬{idx}è¡Œç¼©è¿›ä¸æ˜¯4çš„å€æ•°")

    return issues

def _cyclomatic_complexity(node: ast.AST) -> int:
    """Compute a rough cyclomatic complexity for a function node."""
    complexity = 1
    decision_points = (
        ast.If, ast.For, ast.AsyncFor, ast.While, ast.Try, ast.With, ast.AsyncWith,
        ast.BoolOp, ast.IfExp, ast.Compare, ast.comprehension, ast.ExceptHandler
    )
    for child in ast.walk(node):
        if isinstance(child, decision_points):
            complexity += 1
    return complexity

def _max_nesting_depth(node: ast.AST, depth: int = 0) -> int:
    """Approximate nesting depth for control-flow statements."""
    control_nodes = (ast.If, ast.For, ast.AsyncFor, ast.While, ast.With, ast.AsyncWith, ast.Try)
    max_depth = depth
    for child in ast.iter_child_nodes(node):
        child_depth = depth + 1 if isinstance(child, control_nodes) else depth
        max_depth = max(max_depth, _max_nesting_depth(child, child_depth))
    return max_depth

def _extract_target_names(target: ast.AST) -> Set[str]:
    """Collect variable names introduced by assignment targets."""
    names: Set[str] = set()
    if isinstance(target, ast.Name):
        names.add(target.id)
    elif isinstance(target, (ast.Tuple, ast.List)):
        for elt in target.elts:
            names.update(_extract_target_names(elt))
    elif isinstance(target, ast.Starred):
        names.update(_extract_target_names(target.value))
    return names

def _collect_defined_names(tree: ast.AST) -> Set[str]:
    """Collect names that are defined within the AST (assignments, defs, imports, etc.)."""
    defined: Set[str] = set(BUILTIN_SYMBOLS)

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            defined.add(node.name)

            def _register_args(args: ast.arguments):
                for arg in chain(args.posonlyargs, args.args, args.kwonlyargs):
                    defined.add(arg.arg)
                if args.vararg:
                    defined.add(args.vararg.arg)
                if args.kwarg:
                    defined.add(args.kwarg.arg)

            _register_args(node.args)
        elif isinstance(node, ast.Lambda):
            for arg in node.args.args + node.args.kwonlyargs:
                defined.add(arg.arg)
            if node.args.vararg:
                defined.add(node.args.vararg.arg)
            if node.args.kwarg:
                defined.add(node.args.kwarg.arg)
        elif isinstance(node, ast.ClassDef):
            defined.add(node.name)
        elif isinstance(node, ast.Assign):
            for target in node.targets:
                defined.update(_extract_target_names(target))
        elif isinstance(node, ast.AnnAssign):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.AugAssign):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, (ast.For, ast.AsyncFor)):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.comprehension):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.With):
            for item in node.items:
                if item.optional_vars:
                    defined.update(_extract_target_names(item.optional_vars))
        elif isinstance(node, ast.AsyncWith):
            for item in node.items:
                if item.optional_vars:
                    defined.update(_extract_target_names(item.optional_vars))
        elif isinstance(node, ast.NamedExpr):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.ExceptHandler):
            if isinstance(node.name, str):
                defined.add(node.name)
        elif isinstance(node, ast.Import):
            for alias in node.names:
                defined.add(alias.asname or alias.name.split('.')[0])
        elif isinstance(node, ast.ImportFrom):
            for alias in node.names:
                if alias.name == '*':
                    continue
                defined.add(alias.asname or alias.name)

    return defined

def _detect_name_issues(tree: ast.AST) -> Dict[str, List[int]]:
    """Find names that are referenced but never defined/imported."""
    defined = _collect_defined_names(tree)
    issues: Dict[str, set] = {}

    for node in ast.walk(tree):
        if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):
            if node.id not in defined:
                issues.setdefault(node.id, set()).add(getattr(node, "lineno", 0) or 0)

    return {name: sorted(lines) for name, lines in issues.items() if lines}

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

class EnhancedPDFHandbook:
    """å¢å¼ºç‰ˆPDFå¤„ç†å™¨ï¼Œæ”¯æŒç²¾ç¡®å†…å®¹æå–ã€å›¾ç‰‡è¯†åˆ«å’Œæ£€ç´¢"""
    
    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path
        self.doc = None
        self.content_index = {}  # å…³é”®è¯åˆ°ä½ç½®çš„ç´¢å¼•
        self.image_index = {}    # å›¾ç‰‡åˆ°å†…å®¹çš„æ˜ å°„
        self.sections = {}       # ç« èŠ‚ç»“æ„
        self.text_cache = {}     # é¡µé¢æ–‡æœ¬ç¼“å­˜
        self.images_cache = {}   # å›¾ç‰‡ç¼“å­˜
        self.load_pdf()
        
    def load_pdf(self):
        """åŠ è½½å¹¶ç´¢å¼•PDFå†…å®¹"""
        try:
            if not os.path.exists(self.pdf_path):
                logger.warning(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {self.pdf_path}")
                return
            
            self.doc = fitz.open(self.pdf_path)
            self._extract_content()
            self._build_index()
            self._extract_and_save_images()
            logger.info(f"PDFåŠ è½½æˆåŠŸ: {len(self.doc)}é¡µ, å›¾ç‰‡{len(self.images_cache)}å¼ ")
            
        except Exception as e:
            logger.error(f"åŠ è½½PDFå¤±è´¥: {e}")
    
    def _extract_content(self):
        """æå–PDFæ–‡æœ¬å†…å®¹å¹¶å»ºç«‹ç»“æ„"""
        for page_num in range(len(self.doc)):
            page = self.doc[page_num]
            text = page.get_text("text")
            self.text_cache[page_num] = text
            
            # æå–ç« èŠ‚æ ‡é¢˜
            if page_num == 0:
                self._parse_sections_from_text(text)
    
    def _parse_sections_from_text(self, text: str):
        """ä»æ–‡æœ¬ä¸­è§£æç« èŠ‚ç»“æ„"""
        lines = text.split('\n')
        current_section = "ç®€ä»‹"
        section_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ”¹è¿›çš„ç« èŠ‚æ£€æµ‹
            if self._is_section_title(line):
                if section_lines:
                    self.sections[current_section] = '\n'.join(section_lines)
                current_section = line
                section_lines = []
            else:
                section_lines.append(line)
        
        if section_lines:
            self.sections[current_section] = '\n'.join(section_lines)
    
    def _is_section_title(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç« èŠ‚æ ‡é¢˜"""
        patterns = [
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡]',
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€',
            r'^\d+\.\d+',
            r'^[A-Z][A-Z\s]+$',
            r'^#+ ',
            r'^[ã€\[](.*?)[ã€‘\]]$'
        ]
        
        for pattern in patterns:
            if re.match(pattern, text):
                return True
        
        # é•¿åº¦å’Œå†…å®¹åˆ¤æ–­
        if len(text) < 50 and any(keyword in text.lower() for keyword in 
                                 ['æ¦‚è¿°', 'ç®€ä»‹', 'åŸºç¡€', 'è¿›é˜¶', 'é«˜çº§', 'æ€»ç»“', 'é™„å½•']):
            return True
        
        return False
    
    def _build_section_index(self):
        """å»ºç«‹ç« èŠ‚ç´¢å¼•"""
        for section, content in self.sections.items():
            # æå–å…³é”®è¯
            keywords = self._extract_keywords(content)
            for keyword in keywords:
                if keyword not in self.content_index:
                    self.content_index[keyword] = []
                self.content_index[keyword].append({
                    'section': section,
                    'content': content[:500],  # æˆªå–å‰500å­—ç¬¦
                    'relevance': 'high'
                })
    
    def _extract_keywords(self, text: str, max_keywords: int = 10) -> list:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        # ç§»é™¤åœç”¨è¯
        stop_words = {'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'æœ‰', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'æˆ–', 'ç­‰'}
        
        # æå–åè¯æ€§çŸ­è¯­
        words = re.findall(r'[\u4e00-\u9fff]{2,5}', text) + re.findall(r'\b[a-zA-Z]{3,}\b', text)
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}
        for word in words:
            if word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:max_keywords]]
    
    def _build_index(self):
        """æ„å»ºå…¨æ–‡ç´¢å¼•"""
        python_keywords = {
            'è¯­æ³•', 'å‡½æ•°', 'ç±»', 'å¯¹è±¡', 'æ¨¡å—', 'åŒ…', 'å¼‚å¸¸', 'è£…é¥°å™¨',
            'ç”Ÿæˆå™¨', 'è¿­ä»£å™¨', 'åˆ—è¡¨', 'å­—å…¸', 'é›†åˆ', 'å…ƒç»„', 'å­—ç¬¦ä¸²',
            'æ–‡ä»¶', 'è¾“å…¥è¾“å‡º', 'å¤šçº¿ç¨‹', 'å¼‚æ­¥', 'ç½‘ç»œ', 'æ•°æ®åº“',
            'æµ‹è¯•', 'è°ƒè¯•', 'æ€§èƒ½', 'ä¼˜åŒ–', 'ç®—æ³•', 'æ•°æ®ç»“æ„'
        }
        
        for page_num, text in self.text_cache.items():
            # ä¸ºæ¯ä¸ªå…³é”®è¯å»ºç«‹ç´¢å¼•
            for keyword in python_keywords:
                if keyword in text:
                    if keyword not in self.content_index:
                        self.content_index[keyword] = []
                    
                    # æå–ä¸Šä¸‹æ–‡
                    context = self._get_context(text, keyword, 200)
                    self.content_index[keyword].append({
                        'page': page_num + 1,
                        'context': context,
                        'type': 'keyword_match'
                    })
    
    def _get_context(self, text: str, keyword: str, context_size: int = 200) -> str:
        """è·å–å…³é”®è¯ä¸Šä¸‹æ–‡"""
        pos = text.find(keyword)
        if pos == -1:
            return ""
        
        start = max(0, pos - context_size // 2)
        end = min(len(text), pos + len(keyword) + context_size // 2)
        return text[start:end]
    
    def _extract_and_save_images(self):
        """æå–PDFä¸­çš„å›¾ç‰‡å¹¶ç¼“å­˜"""
        try:
            images_dir = Path("static/images/handbook")
            images_dir.mkdir(parents=True, exist_ok=True)
            
            image_count = 0
            for page_num in range(len(self.doc)):
                page = self.doc[page_num]
                image_list = page.get_images(full=True)
                
                for img_index, img in enumerate(image_list):
                    xref = img[0]
                    base_image = self.doc.extract_image(xref)
                    
                    if base_image:
                        image_bytes = base_image["image"]
                        image_ext = base_image["ext"]
                        
                        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                        image_hash = hashlib.md5(image_bytes).hexdigest()[:8]
                        image_filename = f"page_{page_num+1}_img_{img_index}_{image_hash}.{image_ext}"
                        image_path = images_dir / image_filename
                        
                        # ä¿å­˜å›¾ç‰‡
                        with open(image_path, "wb") as f:
                            f.write(image_bytes)
                        
                        # ç¼“å­˜å›¾ç‰‡ä¿¡æ¯
                        image_key = f"page_{page_num+1}_img_{img_index}"
                        self.images_cache[image_key] = {
                            'path': str(image_path),
                            'page': page_num + 1,
                            'index': img_index,
                            'base64': self._image_to_base64(image_bytes, image_ext),
                            'caption': self._generate_image_caption(page_num, img_index)
                        }
                        
                        # å…³è”å›¾ç‰‡å’Œé™„è¿‘æ–‡æœ¬
                        self._link_image_to_text(page_num, img_index)
                        
                        image_count += 1
            
            logger.info(f"æå–äº† {image_count} å¼ å›¾ç‰‡")
            
        except Exception as e:
            logger.error(f"æå–å›¾ç‰‡å¤±è´¥: {e}")
    
    def _image_to_base64(self, image_bytes: bytes, image_ext: str) -> str:
        """å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64"""
        return base64.b64encode(image_bytes).decode('utf-8')
    
    def _generate_image_caption(self, page_num: int, img_index: int) -> str:
        """ç”Ÿæˆå›¾ç‰‡æ ‡é¢˜"""
        page_text = self.text_cache.get(page_num, "")
        
        # åœ¨å›¾ç‰‡é™„è¿‘æ‰¾ç›¸å…³æ–‡æœ¬ä½œä¸ºæ ‡é¢˜
        lines = page_text.split('\n')
        if lines and len(lines) > 0:
            # ç®€å•è¿”å›é¡µé¢ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
            return lines[0][:50] + "..."
        
        return f"ç¬¬{page_num+1}é¡µ å›¾ç‰‡{img_index+1}"
    
    def _link_image_to_text(self, page_num: int, img_index: int):
        """å°†å›¾ç‰‡ä¸é™„è¿‘æ–‡æœ¬å…³è”"""
        page_text = self.text_cache.get(page_num, "")
        if page_text:
            # æå–é¡µé¢ä¸­çš„å…³é”®è¯
            keywords = self._extract_keywords(page_text, 5)
            image_key = f"page_{page_num+1}_img_{img_index}"
            
            for keyword in keywords:
                if keyword not in self.image_index:
                    self.image_index[keyword] = []
                self.image_index[keyword].append(image_key)
    
    def search_with_images(self, query: str, max_results: int = 5) -> dict:
        """æœç´¢å†…å®¹å¹¶è¿”å›ç›¸å…³å›¾ç‰‡"""
        results = {
            'text_results': [],
            'image_results': [],
            'sections': []
        }
        
        query_lower = query.lower()
        
        # 1. æœç´¢ç« èŠ‚
        for section, content in self.sections.items():
            if query_lower in section.lower() or query_lower in content.lower():
                # æå–ç›¸å…³æ®µè½
                paragraphs = content.split('\n')
                relevant_content = []
                
                for para in paragraphs:
                    if query_lower in para.lower():
                        clean_para = re.sub(r'\s+', ' ', para).strip()
                        if len(clean_para) > 30:
                            relevant_content.append(clean_para)
                
                if relevant_content:
                    results['sections'].append({
                        'title': section,
                        'content': ' '.join(relevant_content[:2]),
                        'full_content': content[:1000]
                    })
        
        # 2. æœç´¢å…³é”®è¯ç´¢å¼•
        for keyword in query_lower.split():
            if keyword in self.content_index:
                for item in self.content_index[keyword][:max_results]:
                    results['text_results'].append({
                        'type': 'keyword',
                        'keyword': keyword,
                        'content': item.get('context', item.get('content', '')),
                        'page': item.get('page', 1),
                        'relevance': item.get('relevance', 'medium')
                    })
        
        # 3. æœç´¢ç›¸å…³å›¾ç‰‡
        for keyword in query_lower.split():
            if keyword in self.image_index:
                for image_key in self.image_index[keyword][:3]:  # æœ€å¤š3å¼ å›¾ç‰‡
                    if image_key in self.images_cache:
                        image_info = self.images_cache[image_key]
                        results['image_results'].append({
                            'key': image_key,
                            'caption': image_info['caption'],
                            'base64': image_info['base64'],
                            'page': image_info['page'],
                            'related_keyword': keyword
                        })
        
        # å¦‚æœæ²¡æœ‰ç›´æ¥ç»“æœï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
        if not results['text_results'] and not results['image_results']:
            results = self._fuzzy_search(query)
        
        return results
    
    def _fuzzy_search(self, query: str) -> dict:
        """æ¨¡ç³Šæœç´¢"""
        results = {
            'text_results': [],
            'image_results': [],
            'sections': []
        }
        
        # åœ¨æ‰€æœ‰æ–‡æœ¬ä¸­æœç´¢
        for page_num, text in self.text_cache.items():
            if query in text.lower():
                context = self._get_context(text, query, 300)
                results['text_results'].append({
                    'type': 'full_text',
                    'content': context,
                    'page': page_num + 1,
                    'relevance': 'medium'
                })
        
        return results
    
    def get_relevant_images(self, topic: str, limit: int = 3) -> list:
        """è·å–ç‰¹å®šä¸»é¢˜çš„ç›¸å…³å›¾ç‰‡"""
        images = []
        
        # ä»å›¾ç‰‡ç´¢å¼•ä¸­æŸ¥æ‰¾
        for keyword in topic.lower().split():
            if keyword in self.image_index:
                for image_key in self.image_index[keyword][:limit]:
                    if image_key in self.images_cache:
                        images.append(self.images_cache[image_key])
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€é¡µçš„å›¾ç‰‡
        if not images:
            for key, img in self.images_cache.items():
                if img['page'] == 1:
                    images.append(img)
                    if len(images) >= limit:
                        break
        
        return images
    
    def get_page_images(self, page_num: int) -> list:
        """è·å–æŒ‡å®šé¡µé¢çš„æ‰€æœ‰å›¾ç‰‡"""
        images = []
        
        for key, img in self.images_cache.items():
            if img['page'] == page_num:
                images.append(img)
        
        return images
    
    def search_exact_content(self, exact_phrase: str) -> list:
        """ç²¾ç¡®çŸ­è¯­æœç´¢"""
        results = []
        
        for page_num, text in self.text_cache.items():
            positions = [m.start() for m in re.finditer(re.escape(exact_phrase), text, re.IGNORECASE)]
            
            for pos in positions[:3]:  # æœ€å¤š3ä¸ªåŒ¹é…
                start = max(0, pos - 100)
                end = min(len(text), pos + len(exact_phrase) + 100)
                context = text[start:end]
                
                results.append({
                    'page': page_num + 1,
                    'position': pos,
                    'context': context,
                    'exact_match': exact_phrase
                })
        
        return results
    
    def generate_citation(self, content: str, max_length: int = 500) -> str:
        """ç”Ÿæˆå¼•ç”¨æ ¼å¼çš„å†…å®¹"""
        if not content:
            return ""
        
        # æŸ¥æ‰¾å†…å®¹åœ¨PDFä¸­çš„ä½ç½®
        for page_num, text in self.text_cache.items():
            if content[:100] in text:
                start_pos = text.find(content[:100])
                if start_pos != -1:
                    return f"ã€ŠPythonèƒŒè®°æ‰‹å†Œã€‹ç¬¬{page_num+1}é¡µ: {content[:max_length]}..."
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›åŸå§‹å†…å®¹
        return content[:max_length] + "..."

class PythonProgrammingAgent:
    def __init__(self):
        self.tools = {
            "code_executor": self.code_executor,
            "syntax_checker": self.syntax_checker,
            "python_documentation": self.python_documentation,
            "code_analyzer": self.code_analyzer,
            "handbook_search": self.handbook_search,
            "enhanced_handbook_search": self.enhanced_handbook_search
        }

        # åˆå§‹åŒ–PDFæ‰‹å†Œ
        pdf_path = os.path.join(os.path.dirname(__file__), 'static', 'PythonèƒŒè®°æ‰‹å†Œ.pdf')
        self.enhanced_handbook = EnhancedPDFHandbook(pdf_path)
        self.handbook = self.enhanced_handbook  # ä¿æŒå‘åå…¼å®¹

        # åˆå§‹åŒ–æ¨¡å‹
        self.llm = None
        try:
            self.llm = ChatOpenAI(
                model="deepseek-chat",
                base_url="https://api.deepseek.com/v1",
                api_key=os.getenv("DEEPSEEK_API_KEY"),
                temperature=0.1
            )
            print("âœ… ä½¿ç”¨ DeepSeek æ¨¡å‹")
        except Exception as e:
            print(f"âŒ DeepSeek åˆå§‹åŒ–å¤±è´¥: {e}")
            try:
                self.llm = ChatOpenAI(
                    model="gpt-3.5-turbo",
                    api_key=os.getenv("OPENAI_API_KEY"),
                    temperature=0.1
                )
                print("âœ… ä½¿ç”¨ OpenAI æ¨¡å‹")
            except Exception as e2:
                print(f"âŒ OpenAI åˆå§‹åŒ–å¤±è´¥: {e2}")
                print("âš ï¸ ä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼ˆæ— APIï¼‰")

        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ï¼Œä¸“é—¨è§£ç­”Pythonç›¸å…³çš„æŠ€æœ¯é—®é¢˜ã€‚ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
1. å‡†ç¡®å›ç­”Pythonè¯­æ³•ã€åº“å‡½æ•°ã€æœ€ä½³å®è·µç­‰é—®é¢˜
2. æä¾›å¯æ‰§è¡Œçš„ä»£ç ç¤ºä¾‹
3. è§£é‡Šä»£ç é€»è¾‘å’ŒåŸç†
4. å¸®åŠ©è°ƒè¯•å’Œä¼˜åŒ–ä»£ç 
5. æä¾›Pythonæœ€æ–°ç‰¹æ€§çš„ä¿¡æ¯
6. å½“å›ç­”æ¶‰åŠã€ŠPythonèƒŒè®°æ‰‹å†Œã€‹çš„å†…å®¹æ—¶ï¼Œä¼˜å…ˆå¼•ç”¨æ‰‹å†Œä¸­çš„æƒå¨è§£é‡Š
7. å¦‚æœæ‰‹å†Œä¸­æœ‰ç›¸å…³å›¾è¡¨æˆ–ç¤ºä¾‹å›¾ç‰‡ï¼Œè¯·åœ¨å›ç­”ä¸­æ˜ç¡®æŒ‡å‡ºå¹¶å¼•ç”¨å›¾ç‰‡å†…å®¹
8. å½“ç”¨æˆ·ä¸Šä¼ ä»£ç æˆ–å›¾è¡¨å›¾ç‰‡æ—¶ï¼Œä»”ç»†åˆ†æå›¾ç‰‡å†…å®¹å¹¶ç»™å‡ºä¸“ä¸šå»ºè®®
9. å¯¹äºå¤æ‚æ¦‚å¿µï¼Œç»“åˆæ‰‹å†Œä¸­çš„å›¾ç¤ºè¿›è¡Œè§£é‡Š

è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
- ç¡®ä¿ä»£ç ç¤ºä¾‹æ˜¯æ­£ç¡®ä¸”å¯è¿è¡Œçš„
- è§£é‡Šè¦æ¸…æ™°æ˜“æ‡‚ï¼Œé€‚åˆä¸åŒæ°´å¹³çš„å¼€å‘è€…
- æä¾›å®é™…åº”ç”¨åœºæ™¯
- æŒ‡å‡ºæ½œåœ¨çš„é™·é˜±å’Œæ³¨æ„äº‹é¡¹
- ä¿æŒå›ç­”çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
- ä½¿ç”¨Markdownæ ¼å¼ç¾åŒ–å›ç­”ï¼Œç‰¹åˆ«æ˜¯ä»£ç å—è¦ç”¨```pythonæ ‡è®°
- å¦‚æœä»æ‰‹å†Œä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ³¨æ˜"æ ¹æ®ã€ŠPythonèƒŒè®°æ‰‹å†Œã€‹ç¬¬Xé¡µ..."
- å¦‚æœæ‰‹å†Œä¸­æœ‰ç›¸å…³å›¾è¡¨ï¼Œè¯·è¯´æ˜"æ‰‹å†Œä¸­çš„å›¾è¡¨å±•ç¤ºäº†..."
- å¯¹äºå¤æ‚æ¦‚å¿µï¼Œå»ºè®®ç”¨æˆ·æŸ¥çœ‹æ‰‹å†Œä¸­çš„å›¾ç¤º"""

    def enhanced_handbook_search(self, query: str) -> str:
        """å¢å¼ºç‰ˆæ‰‹å†Œæœç´¢ï¼ŒåŒ…å«å›¾ç‰‡"""
        try:
            results = self.enhanced_handbook.search_with_images(query)
            
            if not results['text_results'] and not results['image_results']:
                return f"åœ¨ã€ŠPythonèƒŒè®°æ‰‹å†Œã€‹ä¸­æœªæ‰¾åˆ°ä¸'{query}'ç›´æ¥ç›¸å…³çš„å†…å®¹ã€‚"
            
            response = "## ğŸ“š ã€ŠPythonèƒŒè®°æ‰‹å†Œã€‹ç›¸å…³å†…å®¹\n\n"
            
            # æ–‡æœ¬å†…å®¹
            if results.get('text_results'):
                response += "### ğŸ“– ç›¸å…³æ–‡æœ¬å†…å®¹\n\n"
                for i, result in enumerate(results['text_results'][:3], 1):
                    citation = self.enhanced_handbook.generate_citation(result['content'])
                    response += f"{i}. **ç¬¬{result.get('page', '?')}é¡µ** - {citation}\n\n"
            
            # ç« èŠ‚å†…å®¹
            if results.get('sections'):
                response += "### ğŸ“‘ ç›¸å…³ç« èŠ‚\n\n"
                for i, section in enumerate(results['sections'][:2], 1):
                    response += f"{i}. **{section['title']}**\n"
                    response += f"   {section['content'][:200]}...\n\n"
            
            # ç›¸å…³å›¾ç‰‡
            if results.get('image_results'):
                response += "### ğŸ–¼ï¸ ç›¸å…³å›¾è¡¨å’Œç¤ºä¾‹\n\n"
                response += "æ‰‹å†Œä¸­åŒ…å«ä»¥ä¸‹ç›¸å…³å›¾ç¤ºï¼š\n\n"
                for img in results['image_results'][:2]:
                    response += f"- **{img['caption']}** (ç¬¬{img['page']}é¡µ)\n"
                    # åœ¨å“åº”ä¸­æ ‡è®°å›¾ç‰‡ä½ç½®ï¼Œç”±å‰ç«¯å¤„ç†æ˜¾ç¤º
                    response += f"[IMAGE:{img['caption']}]\n{img['base64']}\n[/IMAGE]\n\n"
            
            return response
            
        except Exception as e:
            logger.error(f"å¢å¼ºæ‰‹å†Œæœç´¢å¤±è´¥: {e}")
            return f"æœç´¢æ‰‹å†Œæ—¶å‡ºç°é”™è¯¯: {str(e)}"

    def handbook_search(self, query: str) -> str:
        """ä»PythonèƒŒè®°æ‰‹å†Œä¸­æœç´¢ç›¸å…³ä¿¡æ¯"""
        try:
            # ä½¿ç”¨å¢å¼ºç‰ˆæœç´¢
            return self.enhanced_handbook_search(query)
            
        except Exception as e:
            logger.error(f"æ‰‹å†Œæœç´¢å¤±è´¥: {e}")
            return f"æœç´¢æ‰‹å†Œæ—¶å‡ºç°é”™è¯¯: {str(e)}"

    def code_executor(self, code: str) -> str:
        """æ‰§è¡ŒPythonä»£ç å¹¶è¿”å›ç»“æœã€‚ç”¨äºæµ‹è¯•ä»£ç ç‰‡æ®µæ˜¯å¦æ­£ç¡®è¿è¡Œã€‚"""
        try:
            dangerous_patterns = [
                r'__import__\s*\(', r'eval\s*\(', r'exec\s*\(', r'open\s*\(',
                r'os\.', r'subprocess\.', r'import\s+os', r'import\s+subprocess',
                r'import\s+sys', r'sys\.', r'__builtins__', r'globals\(\)',
                r'locals\(\)', r'rm\s+-', r'del\s+', r'format\s*\(.*\)\.__globals__'
            ]

            for pattern in dangerous_patterns:
                if re.search(pattern, code, re.IGNORECASE):
                    return "é”™è¯¯ï¼šæ£€æµ‹åˆ°å¯èƒ½ä¸å®‰å…¨çš„ä»£ç ï¼Œæ— æ³•æ‰§è¡Œ"

            # é¢å¤–çš„å®‰å…¨æ£€æŸ¥
            if any(keyword in code for keyword in ['__', 'breakpoint', 'compile', 'memoryview']):
                return "é”™è¯¯ï¼šä»£ç åŒ…å«å—é™å…³é”®å­—"

            cleaned_code = self._clean_python_code(code)
            if not cleaned_code:
                return "é”™è¯¯ï¼šä»£ç ä¸ºç©ºæˆ–æ— æ³•å¤„ç†"

            # ä½¿ç”¨subprocessæ‰§è¡Œä»£ç 
            result = subprocess.run(
                [sys.executable, "-c", cleaned_code],
                capture_output=True,
                text=True,
                timeout=10,
                cwd=os.path.dirname(os.path.abspath(__file__)) if __file__ else os.getcwd()
            )

            if result.returncode == 0:
                output = result.stdout.strip()
                return f"âœ… æ‰§è¡ŒæˆåŠŸ:\n{output}" if output else "âœ… ä»£ç æ‰§è¡ŒæˆåŠŸï¼ˆæ— è¾“å‡ºï¼‰"
            else:
                error_msg = result.stderr.strip()
                return f"âŒ æ‰§è¡Œé”™è¯¯:\n{error_msg}"

        except subprocess.TimeoutExpired:
            return "â° é”™è¯¯ï¼šä»£ç æ‰§è¡Œè¶…æ—¶ï¼Œå¯èƒ½å­˜åœ¨æ— é™å¾ªç¯"
        except Exception as e:
            return f"âš ï¸ æ‰§è¡Œå¼‚å¸¸: {str(e)}"

    def syntax_checker(self, code: str) -> str:
        """æ£€æŸ¥Pythonä»£ç çš„è¯­æ³•æ­£ç¡®æ€§ã€‚"""
        cleaned_code = _extract_code_snippet(code)
        if not cleaned_code:
            return "âŒ é”™è¯¯ï¼šä»£ç ä¸ºç©º"

        lines = cleaned_code.splitlines()
        try:
            tree = ast.parse(cleaned_code)
        except SyntaxError as e:
            context = _format_error_context(lines, e.lineno, e.offset)
            error_details = [
                "âŒ è¯­æ³•é”™è¯¯",
                f"- ç±»å‹: {e.msg}",
                f"- ä½ç½®: ç¬¬{e.lineno}è¡Œ, ç¬¬{e.offset}åˆ—" if e.lineno and e.offset else "",
            ]
            if context:
                error_details.append("\nä»£ç ä¸Šä¸‹æ–‡ï¼š")
                error_details.append(context)
            error_details.append("\nå»ºè®®ï¼šæ£€æŸ¥ç¼©è¿›ã€é—æ¼çš„å†’å·ã€æ‹¬å·æˆ–å¼•å·æ˜¯å¦åŒ¹é…ã€‚")
            return "\n".join([line for line in error_details if line])
        except Exception as e:
            return f"âš ï¸ è¯­æ³•æ£€æŸ¥å¼‚å¸¸: {str(e)}"

        structure = _collect_code_structure(tree)
        docstring_issues = _docstring_gaps(tree)
        style_notes = _basic_style_checks(lines)
        name_issues = _detect_name_issues(tree)
        type_total = structure["type_hint_total"] or 1  # é¿å…é™¤0
        type_ratio = structure["type_hint_annotated"] / type_total * 100

        report = [
            "âœ… è¯­æ³•æ£€æŸ¥é€šè¿‡ï¼šæœªå‘ç°è¯­æ³•é”™è¯¯",
            "",
            "ç»“æ„æ¦‚è§ˆ:",
            f"â€¢ æ€»è¡Œæ•°: {len(lines)}",
            f"â€¢ å‡½æ•°æ•°é‡: {len(structure['functions']) + len(structure['async_functions'])}",
            f"â€¢ ç±»æ•°é‡: {len(structure['classes'])}",
            f"â€¢ å¼•å…¥æ¨¡å—: {', '.join(structure['imports']) or 'ï¼ˆæ— ï¼‰'}",
        ]

        if structure["async_functions"]:
            async_names = ", ".join(fn["name"] for fn in structure["async_functions"])
            report.append(f"- å¼‚æ­¥å‡½æ•°: {async_names}")

        report.extend([
            "",
            "ç±»å‹æ³¨è§£è¦†ç›–ç‡:",
            f"â€¢ å‚æ•°æ³¨è§£: {type_ratio:.0f}% ({structure['type_hint_annotated']}/{structure['type_hint_total']})"
            if structure["type_hint_total"]
            else "â€¢ æœªæ£€æµ‹åˆ°å¯ç»Ÿè®¡çš„å‡½æ•°å‚æ•°"
        ])

        if docstring_issues:
            report.extend([
                "",
                "æ–‡æ¡£æç¤º:",
            ])
            for issue in docstring_issues[:5]:
                report.append(f"â€¢ {issue}")
            if len(docstring_issues) > 5:
                report.append("â€¢ ...")

        if name_issues:
            report.extend([
                "",
                "å¯èƒ½çš„åç§°æˆ–ä½œç”¨åŸŸé—®é¢˜:",
            ])
            suggestions = {
                "printf": "æ£€æµ‹åˆ° printfï¼ŒPython ä¸­è¯·ä½¿ç”¨ print()ã€‚",
                "scanf": "æ£€æµ‹åˆ° scanfï¼ŒPython ä¸­å¯ä½¿ç”¨ input()ã€‚",
                "system": "è¯·ç¡®è®¤æ˜¯å¦éœ€è¦ import os åä½¿ç”¨ os.systemã€‚",
            }
            for name in sorted(name_issues.keys())[:6]:
                lines_info = ", ".join(f"ç¬¬{ln}è¡Œ" for ln in name_issues[name][:5])
                extra = suggestions.get(name.lower(), "")
                extra_text = f" {extra}" if extra else ""
                report.append(f"â€¢ {name}: {lines_info}{extra_text}")
            if len(name_issues) > 6:
                report.append("â€¢ ...")

        if style_notes:
            report.extend([
                "",
                "é£æ ¼å»ºè®®ï¼ˆèŠ‚é€‰ï¼‰:",
            ])
            for note in style_notes[:5]:
                report.append(f"â€¢ {note}")
            if len(style_notes) > 5:
                report.append("â€¢ æ›´å¤šé£æ ¼é—®é¢˜è¯·æŸ¥çœ‹å®Œæ•´åˆ†æç»“æœ")

        return "\n".join(report)

    def python_documentation(self, topic: str) -> str:
        """æä¾›Pythonå®˜æ–¹æ–‡æ¡£ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚"""
        docs = {
            "åˆ—è¡¨æ¨å¯¼å¼": {
                "description": "åˆ—è¡¨æ¨å¯¼å¼æä¾›äº†åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼",
                "syntax": "[expression for item in iterable if condition]",
                "examples": [
                    "squares = [x**2 for x in range(5)]  # [0, 1, 4, 9, 16]",
                    "even_squares = [x**2 for x in range(10) if x % 2 == 0]  # [0, 4, 16, 36, 64]",
                    "pairs = [(x, y) for x in range(3) for y in range(3)]  # åµŒå¥—å¾ªç¯"
                ]
            },
            "è£…é¥°å™¨": {
                "description": "è£…é¥°å™¨ç”¨äºä¿®æ”¹å‡½æ•°/ç±»çš„è¡Œä¸º",
                "syntax": "@decorator",
                "examples": [
                    "import time\ndef timer_decorator(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        print(f'è¿è¡Œæ—¶é—´: {time.time()-start:.2f}ç§’')\n        return result\n    return wrapper\n\n@timer_decorator\ndef slow_func():\n    time.sleep(1)\n    return 'å®Œæˆ'"
                ]
            },
            "ç”Ÿæˆå™¨": {
                "description": "ç”Ÿæˆå™¨æ˜¯èŠ‚çœå†…å­˜çš„è¿­ä»£å™¨ï¼Œä½¿ç”¨yieldå…³é”®å­—",
                "syntax": "def generator_func(): yield value",
                "examples": [
                    "def number_gen(n):\n    for i in range(n):\n        yield i",
                    "squares = (x**2 for x in range(5))  # ç”Ÿæˆå™¨è¡¨è¾¾å¼"
                ]
            },
            "ä¸Šä¸‹æ–‡ç®¡ç†å™¨": {
                "description": "ç”¨äºç®¡ç†èµ„æºï¼ˆæ–‡ä»¶/è¿æ¥ï¼‰ï¼Œä½¿ç”¨withè¯­å¥",
                "syntax": "with context_manager as var:",
                "examples": [
                    "with open('file.txt', 'w') as f:\n    f.write('Hello')",
                    "class Timer:\n    def __enter__(self):\n        self.start = time.time()\n        return self\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        print(f'è€—æ—¶: {time.time()-self.start:.2f}ç§’')"
                ]
            },
            "å¼‚å¸¸å¤„ç†": {
                "description": "æ•è·è¿è¡Œæ—¶é”™è¯¯",
                "syntax": "try-except-else-finally",
                "examples": [
                    "try:\n    result = 10 / 2\nexcept ZeroDivisionError:\n    print('ä¸èƒ½é™¤ä»¥é›¶')\nelse:\n    print(f'ç»“æœ: {result}')\nfinally:\n    print('æ¸…ç†å®Œæˆ')"
                ]
            },
            "é¢å‘å¯¹è±¡": {
                "description": "æ”¯æŒç±»/å¯¹è±¡/ç»§æ‰¿",
                "syntax": "class ClassName:",
                "examples": [
                    "class Person:\n    species = 'äººç±»'\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n    def introduce(self):\n        return f'æˆ‘å«{self.name}ï¼Œ{self.age}å²'\n    @classmethod\n    def get_species(cls):\n        return cls.species"
                ]
            }
        }

        topic_lower = topic.lower().strip()
        for key in docs:
            if topic_lower in key.lower():
                doc = docs[key]
                response = f"## {key}\n\n**æè¿°**: {doc['description']}\n\n**è¯­æ³•**: `{doc['syntax']}`\n\n**ç¤ºä¾‹**:\n"
                for example in doc['examples']:
                    response += f"```python\n{example}\n```\n"
                
                # å°è¯•ä»æ‰‹å†Œä¸­æœç´¢ç›¸å…³å†…å®¹
                try:
                    handbook_results = self.enhanced_handbook_search(topic)
                    if "æœªæ‰¾åˆ°" not in handbook_results:
                        response += f"\n## ğŸ“š æ‰‹å†Œç›¸å…³å†…å®¹\n{handbook_results}"
                except:
                    pass
                
                return response

        # å¦‚æœåœ¨é¢„å®šä¹‰æ–‡æ¡£ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»æ‰‹å†Œä¸­æœç´¢
        handbook_result = self.enhanced_handbook_search(topic)
        if "æœªæ‰¾åˆ°" not in handbook_result:
            return handbook_result

        available_topics = "\n".join([f"- {t}" for t in docs.keys()])
        return f"æœªæ‰¾åˆ°'{topic}'çš„æ–‡æ¡£ã€‚å¯ç”¨ä¸»é¢˜ï¼š\n{available_topics}"

    def code_analyzer(self, code: str) -> str:
        """åˆ†æPythonä»£ç ï¼Œæä¾›æ”¹è¿›å»ºè®®å’Œæœ€ä½³å®è·µã€‚"""
        cleaned_code = _extract_code_snippet(code)
        if not cleaned_code:
            return "âŒ é”™è¯¯ï¼šä»£ç ä¸ºç©º"

        lines = cleaned_code.splitlines()
        try:
            tree = ast.parse(cleaned_code)
        except SyntaxError as e:
            context = _format_error_context(lines, e.lineno, e.offset)
            details = [
                "âŒ æ£€æµ‹åˆ°è¯­æ³•é”™è¯¯ï¼Œæ— æ³•ç»§ç»­åˆ†æã€‚",
                f"- é”™è¯¯: {e.msg}",
                f"- ä½ç½®: ç¬¬{e.lineno}è¡Œ, ç¬¬{e.offset}åˆ—" if e.lineno and e.offset else "",
            ]
            if context:
                details.append("\nä»£ç ä¸Šä¸‹æ–‡ï¼š")
                details.append(context)
            details.append("\nè¯·å…ˆä¿®å¤è¯­æ³•é—®é¢˜åå†æ¬¡è¿è¡Œåˆ†æã€‚")
            return "\n".join([line for line in details if line])

        structure = _collect_code_structure(tree)
        docstring_issues = _docstring_gaps(tree)
        style_notes = _basic_style_checks(lines)

        snake_case = re.compile(r'^[a-z_][a-z0-9_]*$')
        pascal_case = re.compile(r'^[A-Z][A-Za-z0-9]+$')

        complexity_notes = []
        maintainability_notes = []
        risk_notes = []
        naming_notes = []

        signature_lookup = {}
        for fn in structure["functions"] + structure["async_functions"]:
            signature_lookup[(fn["name"], fn["lineno"])] = fn

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                signature_meta = signature_lookup.get((node.name, node.lineno))
                args_repr = ", ".join(signature_meta["args"]) if signature_meta else "..."
                signature = f"{'async ' if isinstance(node, ast.AsyncFunctionDef) else ''}{node.name}({args_repr})"
                if signature_meta and signature_meta["returns"]:
                    signature += f" -> {signature_meta['returns']}"

                length = (getattr(node, 'end_lineno', node.lineno) or node.lineno) - node.lineno + 1
                complexity = _cyclomatic_complexity(node)
                depth = _max_nesting_depth(node)

                if complexity >= 12:
                    complexity_notes.append(f"- `{signature}` çš„åœˆå¤æ‚åº¦ä¸º {complexity}ï¼Œå»ºè®®æ‹†è§£é€»è¾‘æˆ–æå–å­å‡½æ•°")
                elif complexity >= 8:
                    complexity_notes.append(f"- `{signature}` çš„åœˆå¤æ‚åº¦ä¸º {complexity}ï¼Œæ¥è¿‘ä¸Šé™ï¼Œæ³¨æ„æ§åˆ¶æ¡ä»¶åˆ†æ”¯")

                if length > 80:
                    maintainability_notes.append(f"- `{signature}` é•¿åº¦ä¸º {length} è¡Œï¼Œå»ºè®®æ‹†åˆ†ä¸ºæ›´å°çš„å‡½æ•°")
                elif length > 40:
                    maintainability_notes.append(f"- `{signature}` é•¿åº¦ä¸º {length} è¡Œï¼Œå¯è€ƒè™‘æå–å…¬å…±é€»è¾‘")

                if depth > 4:
                    maintainability_notes.append(f"- `{signature}` çš„åµŒå¥—æ·±åº¦è¾¾åˆ° {depth} å±‚ï¼Œå»ºè®®é™ä½åµŒå¥—")

                # å¯å˜é»˜è®¤å‚æ•°
                mutable_nodes = (ast.List, ast.Dict, ast.Set)
                defaults = list(node.args.defaults) + [d for d in node.args.kw_defaults if d]
                if any(isinstance(default, mutable_nodes) for default in defaults):
                    risk_notes.append(f"- `{signature}` ä½¿ç”¨å¯å˜å¯¹è±¡ä½œä¸ºé»˜è®¤å‚æ•°ï¼Œå¯èƒ½å¼•å‘å…±äº«çŠ¶æ€é—®é¢˜")

                # å‘½åè§„èŒƒ
                if not snake_case.match(node.name):
                    naming_notes.append(f"- å‡½æ•° `{node.name}` å»ºè®®ä½¿ç”¨ snake_case å‘½å")

            elif isinstance(node, ast.ClassDef):
                if not pascal_case.match(node.name):
                    naming_notes.append(f"- ç±» `{node.name}` å»ºè®®ä½¿ç”¨å¸•æ–¯å¡å‘½åï¼ˆå¦‚ `MyClass`ï¼‰")

            elif isinstance(node, ast.ExceptHandler):
                if node.type is None:
                    risk_notes.append("- æ£€æµ‹åˆ°è£¸ `except:`ï¼Œè¯·æ•è·å…·ä½“å¼‚å¸¸ç±»å‹")
                elif isinstance(node.type, ast.Name) and node.type.id in ("Exception", "BaseException"):
                    risk_notes.append("- æ•è·äº†è¿‡äºå®½æ³›çš„å¼‚å¸¸ `Exception/BaseException`ï¼Œå»ºè®®æ›´ç²¾ç¡®")

            elif isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name) and node.func.id in {"eval", "exec"}:
                    risk_notes.append(f"- ä½¿ç”¨ `{node.func.id}` å¯èƒ½å¸¦æ¥å®‰å…¨é£é™©")
                elif isinstance(node.func, ast.Attribute):
                    attr = node.func.attr
                    owner = getattr(node.func.value, 'id', None)
                    if owner == 'os' and attr in {'system', 'popen'}:
                        risk_notes.append(f"- è°ƒç”¨ `os.{attr}` å¯èƒ½æ‰§è¡Œç³»ç»Ÿå‘½ä»¤ï¼Œè¯·ç¡®è®¤å®‰å…¨æ€§")
                    if owner == 'subprocess' and attr in {'Popen', 'call', 'run'}:
                        risk_notes.append("- ä½¿ç”¨ `subprocess` æ‰§è¡Œå¤–éƒ¨å‘½ä»¤ï¼Œè¯·ç¡®ä¿å‚æ•°å®‰å…¨")

            elif isinstance(node, ast.Global):
                risk_notes.append(f"- åœ¨ç¬¬{node.lineno}è¡Œä½¿ç”¨ `global` ï¼Œå»ºè®®é€šè¿‡å‚æ•°æˆ–è¿”å›å€¼ä¼ é€’æ•°æ®")

        todo_lines = [idx for idx, line in enumerate(lines, 1) if 'TODO' in line or 'FIXME' in line]
        if todo_lines:
            maintainability_notes.append(f"- æ£€æµ‹åˆ°æœªå®Œæˆæ ‡è®°ï¼ˆTODO/FIXMEï¼‰ä½äºè¡Œ: {', '.join(map(str, todo_lines[:5]))}")

        pattern_checks = [
            (r'from\s+\w+\s+import\s+\*', "- é¿å… `from module import *`ï¼Œå¯èƒ½æ±¡æŸ“å‘½åç©ºé—´"),
            (r'while\s+True\s*:', "- `while True` è¯·ç¡®ä¿å­˜åœ¨é€€å‡ºæ¡ä»¶"),
        ]
        for pattern, message in pattern_checks:
            if re.search(pattern, cleaned_code):
                risk_notes.append(message)

        report = ["ä»£ç åˆ†æç»“æœï¼ˆä¸“ä¸šå¢å¼ºç‰ˆï¼‰"]
        report.extend([
            "",
            "ç»“æ„æ¦‚è§ˆ:",
            f"â€¢ è¡Œæ•°: {len(lines)}",
            f"â€¢ å‡½æ•°/å¼‚æ­¥å‡½æ•°: {len(structure['functions']) + len(structure['async_functions'])}",
            f"â€¢ ç±»: {len(structure['classes'])}",
            f"â€¢ å¯¼å…¥: {', '.join(structure['imports']) or 'ï¼ˆæ— ï¼‰'}",
        ])

        if structure["type_hint_total"]:
            ratio = structure["type_hint_annotated"] / structure["type_hint_total"] * 100
            report.append(
                f"â€¢ å‚æ•°ç±»å‹æ³¨è§£è¦†ç›–ç‡: {ratio:.0f}% ({structure['type_hint_annotated']}/{structure['type_hint_total']})")

        if complexity_notes:
            report.extend([
                "",
                "å¤æ‚åº¦ä¸å¯ç»´æŠ¤æ€§:",
                *complexity_notes[:6]
            ])
        if maintainability_notes:
            if "å¤æ‚åº¦ä¸å¯ç»´æŠ¤æ€§:" not in report:
                report.extend(["", "å¤æ‚åº¦ä¸å¯ç»´æŠ¤æ€§:"])
            for note in maintainability_notes[:6]:
                report.append(note)

        if docstring_issues:
            report.extend([
                "",
                "æ–‡æ¡£ä¸å¯è¯»æ€§:",
            ])
            for issue in docstring_issues[:6]:
                report.append(f"â€¢ {issue}")

        if naming_notes:
            report.extend([
                "",
                "å‘½åè§„èŒƒ:",
            ])
            for note in naming_notes[:5]:
                report.append(f"â€¢ {note}")

        if style_notes:
            report.extend([
                "",
                "é£æ ¼å»ºè®®ï¼ˆèŠ‚é€‰ï¼‰:",
            ])
            for note in style_notes[:6]:
                report.append(f"â€¢ {note}")

        if risk_notes:
            report.extend([
                "",
                "æ½œåœ¨é£é™©:",
            ])
            for note in risk_notes[:6]:
                report.append(f"â€¢ {note}")

        if not any([complexity_notes, maintainability_notes, docstring_issues, naming_notes, style_notes, risk_notes]):
            report.extend([
                "",
                "âœ… æœªå‘ç°æ˜æ˜¾çš„é£æ ¼æˆ–è´¨é‡é—®é¢˜ï¼Œä»£ç æ•´ä½“è‰¯å¥½ã€‚"
            ])

        # æ·»åŠ æ‰‹å†Œç›¸å…³å†…å®¹
        try:
            # ä»ä»£ç ä¸­æå–å…³é”®è¯
            keywords = self._extract_code_keywords(cleaned_code)
            if keywords:
                handbook_content = ""
                for keyword in keywords[:2]:  # å–å‰2ä¸ªå…³é”®è¯
                    handbook_result = self.enhanced_handbook_search(keyword)
                    if "æœªæ‰¾åˆ°" not in handbook_result:
                        handbook_content += f"\n\nå…³äº **{keyword}** çš„æ‰‹å†Œå‚è€ƒï¼š\n{handbook_result}"
                
                if handbook_content:
                    report.append("\n" + "="*50)
                    report.append("ğŸ“š ç›¸å…³æ‰‹å†Œå†…å®¹")
                    report.append(handbook_content)
        except Exception as e:
            logger.error(f"æ·»åŠ æ‰‹å†Œå†…å®¹å¤±è´¥: {e}")

        return "\n".join(report)

    def _clean_python_code(self, code: str) -> str:
        """æ¸…ç†è¦æ‰§è¡Œçš„Pythonä»£ç """
        code = re.sub(r'```python\s*|\s*```', '', code).strip()
        if not code:
            return ""

        lines = code.split('\n')
        last_line = lines[-1].strip()

        # å¯¹å­¤ç«‹è¡¨è¾¾å¼æ·»åŠ printï¼ˆä»…åœ¨å®‰å…¨çš„æƒ…å†µä¸‹ï¼‰
        if (last_line and not last_line.startswith(
                (' ', '\t', 'def ', 'class ', 'import ', 'from ', 'if ', 'for ', 'while ', 'with ', '#', 'print(',
                 'return', 'yield', 'raise', 'try', 'except', 'finally'))
                and '=' not in last_line
                and not last_line.endswith((':', ';'))
                and not any(keyword in last_line for keyword in ['lambda', 'async', 'await'])):
            lines[-1] = f"print({last_line})"

        return '\n'.join(lines)

    def _extract_code_keywords(self, code: str) -> List[str]:
        """ä»ä»£ç ä¸­æå–å…³é”®è¯"""
        # å¸¸è§çš„Pythonå…³é”®å­—å’Œåº“
        python_keywords = {
            'def', 'class', 'import', 'from', 'as', 'try', 'except', 'finally',
            'with', 'async', 'await', 'yield', 'lambda', 'global', 'nonlocal'
        }
        
        # å¸¸è§çš„åº“å
        common_libs = {
            'numpy', 'pandas', 'matplotlib', 'requests', 'flask', 'django',
            'tensorflow', 'pytorch', 'sklearn', 'sqlalchemy', 'json', 'csv'
        }
        
        keywords = set()
        
        # æå–å¯¼å…¥çš„åº“
        import_lines = re.findall(r'^(?:import|from)\s+([a-zA-Z0-9_\.]+)', code, re.MULTILINE)
        for lib in import_lines:
            lib_name = lib.split('.')[0]
            if lib_name in common_libs:
                keywords.add(lib_name)
        
        # æå–å‡½æ•°å’Œç±»å
        func_names = re.findall(r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', code)
        class_names = re.findall(r'class\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:\(|:)', code)
        
        keywords.update(func_names)
        keywords.update(class_names)
        
        # æ·»åŠ ä»£ç ä¸­å‡ºç°çš„Pythonå…³é”®å­—
        for keyword in python_keywords:
            if keyword in code:
                keywords.add(keyword)
        
        return list(keywords)

    def _detect_tool_usage(self, question: str) -> dict:
        """æ£€æµ‹ç”¨æˆ·é—®é¢˜æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·"""
        question_lower = question.lower()

        tool_usage = {
            "use_tool": False,
            "tool_name": None,
            "tool_input": None
        }

        # æ£€æµ‹åŸºç¡€æ¦‚å¿µé—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨æ‰‹å†Œæœç´¢
        basic_concepts = ['æ˜¯ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'æ¦‚å¿µ', 'ä»‹ç»', 'è®²è§£', 'è¯´æ˜', 'å«ä¹‰']
        if any(concept in question_lower for concept in basic_concepts):
            # æå–å…³é”®è¯
            words = question_lower.split()
            keywords = [word for word in words if len(word) > 2 and word not in ['python', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€æ ·']]
            if keywords:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "enhanced_handbook_search",
                    "tool_input": ' '.join(keywords[:3])  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
                })

        # æ£€æµ‹ä»£ç æ‰§è¡Œ
        elif any(word in question_lower for word in ['æ‰§è¡Œ', 'è¿è¡Œ', 'è¿è¡Œä»£ç ', 'æ‰§è¡Œä»£ç ', 'test', 'run']):
            code_match = re.search(r'```python\s*(.*?)\s*```', question, re.DOTALL)
            if code_match:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "code_executor",
                    "tool_input": code_match.group(1)
                })

        # æ£€æµ‹è¯­æ³•æ£€æŸ¥
        elif any(word in question_lower for word in ['è¯­æ³•', 'è¯­æ³•æ£€æŸ¥', 'è¯­æ³•é”™è¯¯', 'syntax']):
            code_match = re.search(r'```python\s*(.*?)\s*```', question, re.DOTALL)
            if code_match:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "syntax_checker",
                    "tool_input": code_match.group(1)
                })

        # æ£€æµ‹ä»£ç åˆ†æ
        elif any(word in question_lower for word in ['åˆ†æ', 'ä¼˜åŒ–', 'æ”¹è¿›', 'ä»£ç åˆ†æ', 'analyze']):
            code_match = re.search(r'```python\s*(.*?)\s*```', question, re.DOTALL)
            if code_match:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "code_analyzer",
                    "tool_input": code_match.group(1)
                })

        # æ£€æµ‹æ–‡æ¡£æŸ¥è¯¢
        elif any(word in question_lower for word in ['æ–‡æ¡£', 'è¯´æ˜', 'ä»‹ç»', 'ä»€ä¹ˆæ˜¯', 'documentation']):
            for topic in ['åˆ—è¡¨æ¨å¯¼å¼', 'è£…é¥°å™¨', 'ç”Ÿæˆå™¨', 'ä¸Šä¸‹æ–‡ç®¡ç†å™¨', 'å¼‚å¸¸å¤„ç†', 'é¢å‘å¯¹è±¡', 'å¼‚æ­¥ç¼–ç¨‹',
                          'ç±»å‹æ³¨è§£']:
                if topic in question:
                    tool_usage.update({
                        "use_tool": True,
                        "tool_name": "python_documentation",
                        "tool_input": topic
                    })
                    break

        return tool_usage

    def _should_search_handbook(self, question: str) -> bool:
        """åˆ¤æ–­é—®é¢˜æ˜¯å¦éœ€è¦æœç´¢æ‰‹å†Œ"""
        question_lower = question.lower()
        
        # åŸºç¡€æ¦‚å¿µé—®é¢˜
        basic_concepts = [
            'æ˜¯ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'æ¦‚å¿µ', 'ä»‹ç»', 'è®²è§£', 'è¯´æ˜', 'å«ä¹‰',
            'æ€ä¹ˆç†è§£', 'å¦‚ä½•ç†è§£', 'ä»€ä¹ˆæ„æ€', 'æœ‰ä»€ä¹ˆåŒºåˆ«', 'æœ‰ä»€ä¹ˆä¸åŒ',
            'ä¼˜ç‚¹', 'ç¼ºç‚¹', 'ç‰¹ç‚¹', 'ç‰¹å¾', 'ç‰¹æ€§'
        ]
        
        # å…·ä½“æŠ€æœ¯é—®é¢˜
        technical_terms = [
            'è£…é¥°å™¨', 'ç”Ÿæˆå™¨', 'è¿­ä»£å™¨', 'ä¸Šä¸‹æ–‡ç®¡ç†å™¨', 'å…ƒç±»', 'æè¿°ç¬¦',
            'GIL', 'åƒåœ¾å›æ”¶', 'å†…å­˜ç®¡ç†', 'å¤šçº¿ç¨‹', 'å¤šè¿›ç¨‹', 'åç¨‹',
            'å¼‚æ­¥', 'await', 'async', 'åˆ—è¡¨æ¨å¯¼', 'å­—å…¸æ¨å¯¼', 'é›†åˆæ¨å¯¼',
            'lambda', 'é—­åŒ…', 'ä½œç”¨åŸŸ', 'å‘½åç©ºé—´', 'æ¨¡å—', 'åŒ…'
        ]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŸºç¡€æ¦‚å¿µé—®é¢˜
        for concept in basic_concepts:
            if concept in question_lower:
                return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“æŠ€æœ¯æœ¯è¯­
        for term in technical_terms:
            if term in question_lower:
                return True
        
        return False

    def _get_relevant_handbook_content(self, question: str) -> Optional[str]:
        """è·å–ç›¸å…³çš„æ‰‹å†Œå†…å®¹"""
        try:
            # æå–é—®é¢˜ä¸­çš„å…³é”®è¯
            keywords = re.findall(r'[\u4e00-\u9fff]{2,5}|[a-zA-Z]{3,}', question)
            
            for keyword in keywords:
                if len(keyword) >= 2:  # è‡³å°‘2ä¸ªå­—ç¬¦
                    result = self.enhanced_handbook_search(keyword)
                    if "æœªæ‰¾åˆ°" not in result:
                        return result
            
            return None
        except:
            return None

    def _integrate_handbook_content(self, base_answer: str, handbook_content: str) -> str:
        """å°†æ‰‹å†Œå†…å®¹æ•´åˆåˆ°å›ç­”ä¸­"""
        # ç®€å•çš„æ•´åˆï¼šåœ¨å›ç­”å¼€å¤´æ·»åŠ æ‰‹å†Œå†…å®¹
        integration = f"""
## ğŸ” æ‰‹å†Œå‚è€ƒ

{handbook_content}

---

## ğŸ’¡ æˆ‘çš„è§£ç­”

{base_answer}

> ğŸ“š ä»¥ä¸Šå›ç­”å‚è€ƒäº†ã€ŠPythonèƒŒè®°æ‰‹å†Œã€‹ä¸­çš„ç›¸å…³å†…å®¹ï¼Œç¡®ä¿äº†è§£é‡Šçš„å‡†ç¡®æ€§å’Œæƒå¨æ€§ã€‚
"""
        return integration

    def ask_question(self, question: str) -> str:
        """å‘æ™ºèƒ½ä½“æé—®å…³äºPythonç¼–ç¨‹çš„é—®é¢˜"""
        try:
            # æ£€æµ‹æ˜¯å¦ä¸ºéœ€è¦æ‰‹å†Œå¼•ç”¨çš„é—®é¢˜
            should_search_handbook = self._should_search_handbook(question)
            
            # å…ˆè·å–æ‰‹å†Œå†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            handbook_content = None
            if should_search_handbook and self.enhanced_handbook:
                handbook_content = self._get_relevant_handbook_content(question)
            
            # å‡†å¤‡æé—®å†…å®¹
            enhanced_question = question
            if handbook_content:
                # å°†æ‰‹å†Œå†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡æ·»åŠ åˆ°é—®é¢˜ä¸­
                enhanced_question = f"""
ç”¨æˆ·é—®é¢˜: {question}

æ ¹æ®ã€ŠPythonèƒŒè®°æ‰‹å†Œã€‹ç›¸å…³å†…å®¹:
{handbook_content}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œç¡®ä¿å›ç­”å‡†ç¡®ä¸”å¼•ç”¨æ‰‹å†Œä¸­çš„æƒå¨è§£é‡Šã€‚
"""
            
            # æ£€æµ‹æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
            tool_info = self._detect_tool_usage(question)

            if tool_info["use_tool"]:
                tool_name = tool_info["tool_name"]
                tool_input = tool_info["tool_input"]

                if tool_name in self.tools:
                    print(f"ğŸ”§ ä½¿ç”¨å·¥å…·: {tool_name}")
                    tool_result = self.tools[tool_name](tool_input)

                    # å¦‚æœæœ‰LLMï¼Œè®©LLMæ¥è§£é‡Šå·¥å…·ç»“æœ
                    if self.llm:
                        enhanced_prompt = f"""ç”¨æˆ·çš„é—®é¢˜: {question}

å·¥å…·æ‰§è¡Œç»“æœ:
{tool_result}

è¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœï¼Œç»™ç”¨æˆ·ä¸€ä¸ªå®Œæ•´ã€ä¸“ä¸šçš„å›ç­”ã€‚ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨Markdownæ ¼å¼ã€‚å¦‚æœå¯èƒ½ï¼Œå¼•ç”¨ã€ŠPythonèƒŒè®°æ‰‹å†Œã€‹ä¸­çš„ç›¸å…³å†…å®¹ã€‚"""

                        messages = [
                            SystemMessage(
                                content="ä½ æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹åŠ©æ‰‹ï¼Œè¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœå’Œã€ŠPythonèƒŒè®°æ‰‹å†Œã€‹ç»™ç”¨æˆ·æä¾›ä¸“ä¸šã€å®Œæ•´çš„å›ç­”ã€‚"),
                            HumanMessage(content=enhanced_prompt)
                        ]
                        response = self.llm.invoke(messages)
                        return response.content
                    else:
                        # æ— LLMæ—¶ç›´æ¥è¿”å›å·¥å…·ç»“æœ
                        return f"**å·¥å…·æ‰§è¡Œç»“æœ**:\n\n{tool_result}"

            # å¦‚æœæ²¡æœ‰ä½¿ç”¨å·¥å…·æˆ–è€…å·¥å…·ä½¿ç”¨å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨LLM
            if self.llm:
                messages = [
                    SystemMessage(content=self.system_prompt),
                    HumanMessage(content=enhanced_question)
                ]
                response = self.llm.invoke(messages)
                answer = response.content
                
                # å¦‚æœæ‰‹å†Œæœ‰ç›¸å…³å†…å®¹ä¸”æ²¡åŒ…å«åœ¨å›ç­”ä¸­ï¼Œæ·»åŠ å¼•ç”¨
                if handbook_content and "ã€ŠPythonèƒŒè®°æ‰‹å†Œã€‹" not in answer:
                    answer = self._integrate_handbook_content(answer, handbook_content)
                
                return answer
            else:
                return self._local_answer(question)

        except Exception as e:
            error_msg = f"æé—®é”™è¯¯: {str(e)}"
            print(f"Error: {error_msg}")
            return f"âš ï¸ {error_msg}\nè¯·æ£€æŸ¥APIå¯†é’¥æˆ–ç½‘ç»œè¿æ¥"

    def _local_answer(self, question: str) -> str:
        """æ— APIæ—¶çš„æœ¬åœ°å›ç­”"""
        q = question.lower()

        # å°è¯•ä½¿ç”¨å·¥å…·
        tool_info = self._detect_tool_usage(question)
        if tool_info["use_tool"] and tool_info["tool_name"] in self.tools:
            return self.tools[tool_info["tool_name"]](tool_info["tool_input"])

        # å°è¯•ä»æ‰‹å†Œä¸­æœç´¢
        try:
            words = q.split()
            keywords = [word for word in words if len(word) > 2]
            if keywords:
                handbook_result = self.enhanced_handbook_search(' '.join(keywords[:2]))
                if "æœªæ‰¾åˆ°" not in handbook_result:
                    return handbook_result
        except:
            pass

        # æœ¬åœ°çŸ¥è¯†åº“
        if any(w in q for w in ['åˆ—è¡¨æ¨å¯¼', 'list comprehension']):
            return self.python_documentation("åˆ—è¡¨æ¨å¯¼å¼")
        elif any(w in q for w in ['è£…é¥°å™¨', 'decorator']):
            return self.python_documentation("è£…é¥°å™¨")
        elif any(w in q for w in ['ç”Ÿæˆå™¨', 'generator']):
            return self.python_documentation("ç”Ÿæˆå™¨")
        elif any(w in q for w in ['ä¸Šä¸‹æ–‡ç®¡ç†', 'context manager', 'with']):
            return self.python_documentation("ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
        elif any(w in q for w in ['å¼‚å¸¸', 'exception', 'try', 'except']):
            return self.python_documentation("å¼‚å¸¸å¤„ç†")
        elif any(w in q for w in ['ç±»', 'class', 'å¯¹è±¡', 'object', 'é¢å‘å¯¹è±¡']):
            return self.python_documentation("é¢å‘å¯¹è±¡")
        elif any(w in q for w in ['è¯­æ³•', 'syntax', 'æ£€æŸ¥']):
            return "è¯·æä¾›Pythonä»£ç ï¼Œæˆ‘å°†æ£€æŸ¥è¯­æ³•æ­£ç¡®æ€§ï¼ˆç¤ºä¾‹ï¼š\n```python\ndef add(a,b): return a+b\n```ï¼‰"
        elif any(w in q for w in ['æ‰§è¡Œ', 'è¿è¡Œ', 'test', 'run']):
            return "è¯·æä¾›Pythonä»£ç ï¼Œæˆ‘å°†æ‰§è¡Œå¹¶è¿”å›ç»“æœï¼ˆç¤ºä¾‹ï¼š\n```python\nprint([i for i in range(5)])\n```ï¼‰"
        elif any(w in q for w in ['åˆ†æ', 'ä¼˜åŒ–', 'æ”¹è¿›']):
            return "è¯·æä¾›Pythonä»£ç ï¼Œæˆ‘å°†åˆ†æå¹¶ç»™å‡ºæ”¹è¿›å»ºè®®ï¼ˆç¤ºä¾‹ï¼š\n```python\ndef calc():\n    total=0\n    for i in range(10): total+=i\n    print(total)\n```ï¼‰"
        else:
            return "âš ï¸ æœ¬åœ°æ¨¡å¼ä»…æ”¯æŒä»¥ä¸‹ä¸»é¢˜ï¼š\n- åˆ—è¡¨æ¨å¯¼å¼ã€è£…é¥°å™¨ã€ç”Ÿæˆå™¨\n- ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€å¼‚å¸¸å¤„ç†ã€é¢å‘å¯¹è±¡\n- ä»£ç è¯­æ³•æ£€æŸ¥ã€æ‰§è¡Œã€åˆ†æä¼˜åŒ–\nå¦‚éœ€æ›´å¤šå›ç­”ï¼Œè¯·é…ç½®DeepSeek/OpenAI APIå¯†é’¥"
