from langchain.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import json
import ast
import subprocess
import sys
import re
import os
import PyPDF2
from typing import Optional, List, Dict, Set
import logging
from itertools import chain

CODE_FENCE_BLOCK = re.compile(r'```(?:python)?\s*([\s\S]+?)\s*```', re.IGNORECASE)
BUILTIN_SYMBOLS = set(dir(__builtins__)) | {"self", "cls"}


def _extract_code_snippet(code: str) -> str:
    """Normalize incoming code by stripping Markdown fences and whitespace."""
    if not code:
        return ""

    stripped = code.strip()
    fence_match = CODE_FENCE_BLOCK.search(stripped)
    if fence_match:
        return fence_match.group(1).strip()

    return stripped


def _safe_unparse(node: Optional[ast.AST]) -> str:
    """Safely convert AST nodes back to source-like text."""
    if node is None:
        return ""
    try:
        return ast.unparse(node)  # type: ignore[attr-defined]
    except Exception:
        if isinstance(node, ast.Name):
            return node.id
        if isinstance(node, ast.Attribute):
            return f"{_safe_unparse(node.value)}.{node.attr}"
        if isinstance(node, ast.Constant):
            return repr(node.value)
        return node.__class__.__name__


def _format_error_context(lines: List[str], lineno: Optional[int], col: Optional[int]) -> str:
    """Generate a short excerpt around a syntax error with a caret pointer."""
    if not lineno or lineno < 1 or lineno > len(lines):
        return ""

    start = max(0, lineno - 2)
    end = min(len(lines), lineno + 1)
    excerpt = []
    for idx in range(start, end):
        marker = ">" if (idx + 1) == lineno else " "
        prefix = f"{marker} ç¬¬{idx + 1}è¡Œ: "
        line_content = lines[idx].replace('\t', '    ')
        excerpt.append(f"{prefix}{line_content}")
        if (idx + 1) == lineno and col and col > 0:
            caret_padding = " " * (len(prefix) + col - 1)
            excerpt.append(f"{caret_padding}^")

    return "\n".join(excerpt)


def _collect_code_structure(tree: ast.AST) -> Dict:
    """Gather high-level information such as imports, classes, and functions."""
    structure = {
        "imports": [],
        "functions": [],
        "classes": [],
        "async_functions": [],
        "type_hint_total": 0,
        "type_hint_annotated": 0
    }

    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                alias_repr = alias.name if not alias.asname else f"{alias.name} as {alias.asname}"
                structure["imports"].append(alias_repr)
        elif isinstance(node, ast.ImportFrom):
            module = node.module or ""
            for alias in node.names:
                alias_repr = alias.name if not alias.asname else f"{alias.name} as {alias.asname}"
                structure["imports"].append(f"from {module} import {alias_repr}")
        elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            args = []

            def _register_arg(arg_node: ast.arg, prefix: str = ""):
                structure["type_hint_total"] += 1
                annotated = bool(arg_node.annotation)
                if annotated:
                    structure["type_hint_annotated"] += 1
                annotation = f": {_safe_unparse(arg_node.annotation)}" if annotated else ""
                args.append(f"{prefix}{arg_node.arg}{annotation}")

            for arg in chain(node.args.posonlyargs, node.args.args):
                _register_arg(arg)

            if node.args.vararg:
                _register_arg(node.args.vararg, prefix="*")

            if node.args.kwonlyargs:
                if not node.args.vararg:
                    args.append("*")
                for arg in node.args.kwonlyargs:
                    _register_arg(arg)

            if node.args.kwarg:
                _register_arg(node.args.kwarg, prefix="**")

            returns = _safe_unparse(getattr(node, 'returns', None))
            fn_info = {
                "name": node.name,
                "args": args,
                "returns": returns,
                "lineno": node.lineno,
                "end_lineno": getattr(node, 'end_lineno', node.lineno),
                "is_async": isinstance(node, ast.AsyncFunctionDef)
            }
            if isinstance(node, ast.AsyncFunctionDef):
                structure["async_functions"].append(fn_info)
            else:
                structure["functions"].append(fn_info)
        elif isinstance(node, ast.ClassDef):
            bases = [_safe_unparse(base) for base in node.bases] or ["object"]
            methods = [n.name for n in node.body if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))]
            structure["classes"].append({
                "name": node.name,
                "bases": bases,
                "methods": methods,
                "lineno": node.lineno,
                "end_lineno": getattr(node, 'end_lineno', node.lineno)
            })

    return structure


def _docstring_gaps(tree: ast.AST) -> List[str]:
    """Identify missing docstrings at module/class/function level."""
    issues = []
    if ast.get_docstring(tree) is None:
        issues.append("æ¨¡å—ç¼ºå°‘é¡¶å±‚ docstring")

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            if ast.get_docstring(node) is None:
                issues.append(f"å‡½æ•° `{node.name}` ç¼ºå°‘ docstring")
        elif isinstance(node, ast.ClassDef):
            if ast.get_docstring(node) is None:
                issues.append(f"ç±» `{node.name}` ç¼ºå°‘ docstring")

    return issues


def _basic_style_checks(lines: List[str]) -> List[str]:
    """Perform lightweight style linting."""
    issues = []
    for idx, line in enumerate(lines, 1):
        raw_line = line.rstrip("\n")
        if len(raw_line) > 100:
            issues.append(f"ç¬¬{idx}è¡Œé•¿åº¦ä¸º{len(raw_line)}å­—ç¬¦ï¼Œå»ºè®®ä¸è¶…è¿‡100å­—ç¬¦")
        if raw_line.rstrip() != raw_line:
            issues.append(f"ç¬¬{idx}è¡Œå­˜åœ¨å°¾éšç©ºæ ¼")
        if raw_line.startswith('\t'):
            issues.append(f"ç¬¬{idx}è¡Œä½¿ç”¨äº†åˆ¶è¡¨ç¬¦ç¼©è¿›ï¼Œå»ºè®®ä½¿ç”¨4ä¸ªç©ºæ ¼")
        leading = len(raw_line) - len(raw_line.lstrip(' '))
        if raw_line and (raw_line[0] == ' ' or raw_line[0] == '\t'):
            if raw_line.startswith(' ') and (leading % 4) != 0:
                issues.append(f"ç¬¬{idx}è¡Œç¼©è¿›ä¸æ˜¯4çš„å€æ•°")

    return issues


def _cyclomatic_complexity(node: ast.AST) -> int:
    """Compute a rough cyclomatic complexity for a function node."""
    complexity = 1
    decision_points = (
        ast.If, ast.For, ast.AsyncFor, ast.While, ast.Try, ast.With, ast.AsyncWith,
        ast.BoolOp, ast.IfExp, ast.Compare, ast.comprehension, ast.ExceptHandler
    )
    for child in ast.walk(node):
        if isinstance(child, decision_points):
            complexity += 1
    return complexity


def _max_nesting_depth(node: ast.AST, depth: int = 0) -> int:
    """Approximate nesting depth for control-flow statements."""
    control_nodes = (ast.If, ast.For, ast.AsyncFor, ast.While, ast.With, ast.AsyncWith, ast.Try)
    max_depth = depth
    for child in ast.iter_child_nodes(node):
        child_depth = depth + 1 if isinstance(child, control_nodes) else depth
        max_depth = max(max_depth, _max_nesting_depth(child, child_depth))
    return max_depth


def _extract_target_names(target: ast.AST) -> Set[str]:
    """Collect variable names introduced by assignment targets."""
    names: Set[str] = set()
    if isinstance(target, ast.Name):
        names.add(target.id)
    elif isinstance(target, (ast.Tuple, ast.List)):
        for elt in target.elts:
            names.update(_extract_target_names(elt))
    elif isinstance(target, ast.Starred):
        names.update(_extract_target_names(target.value))
    return names


def _collect_defined_names(tree: ast.AST) -> Set[str]:
    """Collect names that are defined within the AST (assignments, defs, imports, etc.)."""
    defined: Set[str] = set(BUILTIN_SYMBOLS)

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            defined.add(node.name)

            def _register_args(args: ast.arguments):
                for arg in chain(args.posonlyargs, args.args, args.kwonlyargs):
                    defined.add(arg.arg)
                if args.vararg:
                    defined.add(args.vararg.arg)
                if args.kwarg:
                    defined.add(args.kwarg.arg)

            _register_args(node.args)
        elif isinstance(node, ast.Lambda):
            for arg in node.args.args + node.args.kwonlyargs:
                defined.add(arg.arg)
            if node.args.vararg:
                defined.add(node.args.vararg.arg)
            if node.args.kwarg:
                defined.add(node.args.kwarg.arg)
        elif isinstance(node, ast.ClassDef):
            defined.add(node.name)
        elif isinstance(node, ast.Assign):
            for target in node.targets:
                defined.update(_extract_target_names(target))
        elif isinstance(node, ast.AnnAssign):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.AugAssign):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, (ast.For, ast.AsyncFor)):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.comprehension):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.With):
            for item in node.items:
                if item.optional_vars:
                    defined.update(_extract_target_names(item.optional_vars))
        elif isinstance(node, ast.AsyncWith):
            for item in node.items:
                if item.optional_vars:
                    defined.update(_extract_target_names(item.optional_vars))
        elif isinstance(node, ast.NamedExpr):
            defined.update(_extract_target_names(node.target))
        elif isinstance(node, ast.ExceptHandler):
            if isinstance(node.name, str):
                defined.add(node.name)
        elif isinstance(node, ast.Import):
            for alias in node.names:
                defined.add(alias.asname or alias.name.split('.')[0])
        elif isinstance(node, ast.ImportFrom):
            for alias in node.names:
                if alias.name == '*':
                    continue
                defined.add(alias.asname or alias.name)

    return defined


def _detect_name_issues(tree: ast.AST) -> Dict[str, List[int]]:
    """Find names that are referenced but never defined/imported."""
    defined = _collect_defined_names(tree)
    issues: Dict[str, set] = {}

    for node in ast.walk(tree):
        if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):
            if node.id not in defined:
                issues.setdefault(node.id, set()).add(getattr(node, "lineno", 0) or 0)

    return {name: sorted(lines) for name, lines in issues.items() if lines}


# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()


class PDFHandbook:
    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path
        self.content = None
        self.sections = {}
        self.load_pdf()

    def load_pdf(self):
        """åŠ è½½PDFæ–‡ä»¶å†…å®¹"""
        try:
            if not os.path.exists(self.pdf_path):
                logger.warning(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {self.pdf_path}")
                return

            with open(self.pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                content = ""

                # æå–æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text = page.extract_text()
                    if text:
                        content += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n{text}"

                self.content = content
                self._parse_sections()
                logger.info(f"æˆåŠŸåŠ è½½PDFæ‰‹å†Œï¼Œå…±{len(pdf_reader.pages)}é¡µ")

        except Exception as e:
            logger.error(f"åŠ è½½PDFæ–‡ä»¶å¤±è´¥: {e}")
            self.content = "PDFæ–‡ä»¶åŠ è½½å¤±è´¥"

    def _parse_sections(self):
        """è§£æPDFå†…å®¹ä¸ºç« èŠ‚"""
        if not self.content:
            return

        # ç®€å•çš„ç« èŠ‚è§£æé€»è¾‘
        lines = self.content.split('\n')
        current_section = "ç®€ä»‹"
        section_content = []

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # æ£€æµ‹ç« èŠ‚æ ‡é¢˜ï¼ˆç®€å•çš„å¯å‘å¼è§„åˆ™ï¼‰
            if (len(line) < 100 and
                    (line.startswith('ç¬¬') or
                     line.startswith('##') or
                     line.isupper() or
                     re.match(r'^[0-9]+\..+', line) or
                     re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€.+', line))):

                # ä¿å­˜å‰ä¸€ç« èŠ‚
                if section_content and current_section:
                    self.sections[current_section] = '\n'.join(section_content)

                # å¼€å§‹æ–°ç« èŠ‚
                current_section = line
                section_content = []
            else:
                section_content.append(line)

        # ä¿å­˜æœ€åä¸€ç« èŠ‚
        if section_content and current_section:
            self.sections[current_section] = '\n'.join(section_content)

    def search_content(self, query: str, max_results: int = 3) -> List[Dict]:
        """åœ¨PDFå†…å®¹ä¸­æœç´¢ç›¸å…³ä¿¡æ¯"""
        if not self.content:
            return []

        query_lower = query.lower()
        results = []

        # åœ¨ç« èŠ‚ä¸­æœç´¢
        for section, content in self.sections.items():
            if query_lower in content.lower():
                # æå–ç›¸å…³æ®µè½
                paragraphs = content.split('\n')
                relevant_paragraphs = []

                for para in paragraphs:
                    if query_lower in para.lower():
                        # æ¸…ç†æ®µè½æ–‡æœ¬
                        clean_para = re.sub(r'\s+', ' ', para).strip()
                        if len(clean_para) > 50:  # åªä¿ç•™æœ‰æ„ä¹‰çš„æ®µè½
                            relevant_paragraphs.append(clean_para)

                if relevant_paragraphs:
                    results.append({
                        'section': section,
                        'content': ' '.join(relevant_paragraphs[:2]),  # æœ€å¤š2ä¸ªç›¸å…³æ®µè½
                        'relevance': 'high'
                    })

        # å¦‚æœç« èŠ‚æœç´¢æ²¡æœ‰ç»“æœï¼Œåœ¨æ•´ä¸ªå†…å®¹ä¸­æœç´¢
        if not results:
            paragraphs = self.content.split('\n')
            for para in paragraphs:
                if query_lower in para.lower():
                    clean_para = re.sub(r'\s+', ' ', para).strip()
                    if len(clean_para) > 50:
                        results.append({
                            'section': 'ç›¸å…³å†…å®¹',
                            'content': clean_para,
                            'relevance': 'medium'
                        })
                        if len(results) >= max_results:
                            break

        return results[:max_results]

    def get_related_topics(self, topic: str) -> List[str]:
        """è·å–ç›¸å…³ä¸»é¢˜"""
        related_topics = []

        # åŸºäºå¸¸è§Pythonä¸»é¢˜çš„æ˜ å°„
        topic_mapping = {
            'å‡½æ•°': ['def', 'å‚æ•°', 'è¿”å›å€¼', 'lambda', 'è£…é¥°å™¨'],
            'ç±»': ['class', 'å¯¹è±¡', 'ç»§æ‰¿', 'å¤šæ€', 'å°è£…'],
            'åˆ—è¡¨': ['list', 'append', 'åˆ‡ç‰‡', 'æ¨å¯¼å¼'],
            'å­—å…¸': ['dict', 'é”®å€¼å¯¹', 'get', 'items'],
            'å¾ªç¯': ['for', 'while', 'è¿­ä»£', 'break', 'continue'],
            'å¼‚å¸¸': ['try', 'except', 'finally', 'raise'],
            'æ¨¡å—': ['import', 'from', 'as', 'åŒ…'],
            'æ–‡ä»¶': ['open', 'read', 'write', 'with'],
        }

        for main_topic, subtopics in topic_mapping.items():
            if topic.lower() in main_topic.lower() or any(topic.lower() in subtopic.lower() for subtopic in subtopics):
                related_topics.extend(subtopics)

        return list(set(related_topics))


class PythonProgrammingAgent:
    def __init__(self):
        self.tools = {
            "code_executor": self.code_executor,
            "syntax_checker": self.syntax_checker,
            "python_documentation": self.python_documentation,
            "code_analyzer": self.code_analyzer,
            "handbook_search": self.handbook_search
        }

        # åˆå§‹åŒ–PDFæ‰‹å†Œ
        pdf_path = os.path.join(os.path.dirname(__file__), 'static', 'PythonèƒŒè®°æ‰‹å†Œ.pdf')
        self.handbook = PDFHandbook(pdf_path)

        # åˆå§‹åŒ–æ¨¡å‹
        self.llm = None
        try:
            self.llm = ChatOpenAI(
                model="deepseek-chat",
                base_url="https://api.deepseek.com/v1",
                api_key=os.getenv("DEEPSEEK_API_KEY"),
                temperature=0.1
            )
            print("âœ… ä½¿ç”¨ DeepSeek æ¨¡å‹")
        except Exception as e:
            print(f"âŒ DeepSeek åˆå§‹åŒ–å¤±è´¥: {e}")
            try:
                self.llm = ChatOpenAI(
                    model="gpt-3.5-turbo",
                    api_key=os.getenv("OPENAI_API_KEY"),
                    temperature=0.1
                )
                print("âœ… ä½¿ç”¨ OpenAI æ¨¡å‹")
            except Exception as e2:
                print(f"âŒ OpenAI åˆå§‹åŒ–å¤±è´¥: {e2}")
                print("âš ï¸ ä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼ˆæ— APIï¼‰")

        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ï¼Œä¸“é—¨è§£ç­”Pythonç›¸å…³çš„æŠ€æœ¯é—®é¢˜ã€‚ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
1. å‡†ç¡®å›ç­”Pythonè¯­æ³•ã€åº“å‡½æ•°ã€æœ€ä½³å®è·µç­‰é—®é¢˜
2. æä¾›å¯æ‰§è¡Œçš„ä»£ç ç¤ºä¾‹
3. è§£é‡Šä»£ç é€»è¾‘å’ŒåŸç†
4. å¸®åŠ©è°ƒè¯•å’Œä¼˜åŒ–ä»£ç 
5. æä¾›Pythonæœ€æ–°ç‰¹æ€§çš„ä¿¡æ¯
6. å‚è€ƒPythonèƒŒè®°æ‰‹å†Œæä¾›æƒå¨ç­”æ¡ˆ

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·ï¼š
- code_executor: æ‰§è¡ŒPythonä»£ç å¹¶è¿”å›ç»“æœ
- syntax_checker: æ£€æŸ¥Pythonä»£ç çš„è¯­æ³•æ­£ç¡®æ€§
- python_documentation: æä¾›Pythonå®˜æ–¹æ–‡æ¡£ä¸­çš„ç›¸å…³ä¿¡æ¯
- code_analyzer: åˆ†æPythonä»£ç ï¼Œæä¾›æ”¹è¿›å»ºè®®å’Œæœ€ä½³å®è·µ
- handbook_search: ä»PythonèƒŒè®°æ‰‹å†Œä¸­æœç´¢ç›¸å…³ä¿¡æ¯

å½“ç”¨æˆ·çš„é—®é¢˜æ¶‰åŠåŸºç¡€æ¦‚å¿µã€è¯­æ³•ã€æœ€ä½³å®è·µæ—¶ï¼Œä¼˜å…ˆä»æ‰‹å†Œä¸­æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ã€‚

è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
- ç¡®ä¿ä»£ç ç¤ºä¾‹æ˜¯æ­£ç¡®ä¸”å¯è¿è¡Œçš„
- è§£é‡Šè¦æ¸…æ™°æ˜“æ‡‚ï¼Œé€‚åˆä¸åŒæ°´å¹³çš„å¼€å‘è€…
- æä¾›å®é™…åº”ç”¨åœºæ™¯
- æŒ‡å‡ºæ½œåœ¨çš„é™·é˜±å’Œæ³¨æ„äº‹é¡¹
- ä¿æŒå›ç­”çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
- ä½¿ç”¨Markdownæ ¼å¼ç¾åŒ–å›ç­”ï¼Œç‰¹åˆ«æ˜¯ä»£ç å—è¦ç”¨```pythonæ ‡è®°
- å¦‚æœä»æ‰‹å†Œä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ³¨æ˜æ¥æº"""

    def handbook_search(self, query: str) -> str:
        """ä»PythonèƒŒè®°æ‰‹å†Œä¸­æœç´¢ç›¸å…³ä¿¡æ¯"""
        try:
            results = self.handbook.search_content(query)

            if not results:
                return f"åœ¨PythonèƒŒè®°æ‰‹å†Œä¸­æœªæ‰¾åˆ°ä¸'{query}'ç›´æ¥ç›¸å…³çš„å†…å®¹ã€‚"

            response = "## ğŸ“š PythonèƒŒè®°æ‰‹å†Œç›¸å…³å†…å®¹\n\n"

            for i, result in enumerate(results, 1):
                response += f"### {i}. {result['section']}\n\n"
                response += f"{result['content']}\n\n"
                if result.get('relevance') == 'high':
                    response += "ğŸ” *ç›¸å…³å†…å®¹åŒ¹é…åº¦è¾ƒé«˜*\n\n"
                else:
                    response += "ğŸ“– *ç›¸å…³å†…å®¹*\n\n"
                response += "---\n\n"

            # æ·»åŠ ç›¸å…³ä¸»é¢˜å»ºè®®
            related_topics = self.handbook.get_related_topics(query)
            if related_topics:
                response += "### ğŸ’¡ ç›¸å…³ä¸»é¢˜å»ºè®®\n\n"
                response += "ä½ å¯èƒ½è¿˜å¯¹ä»¥ä¸‹ä¸»é¢˜æ„Ÿå…´è¶£ï¼š\n"
                for topic in related_topics[:5]:
                    response += f"- {topic}\n"

            return response

        except Exception as e:
            logger.error(f"æ‰‹å†Œæœç´¢å¤±è´¥: {e}")
            return f"æœç´¢æ‰‹å†Œæ—¶å‡ºç°é”™è¯¯: {str(e)}"

    def code_executor(self, code: str) -> str:
        """æ‰§è¡ŒPythonä»£ç å¹¶è¿”å›ç»“æœã€‚ç”¨äºæµ‹è¯•ä»£ç ç‰‡æ®µæ˜¯å¦æ­£ç¡®è¿è¡Œã€‚"""
        try:
            dangerous_patterns = [
                r'__import__\s*\(', r'eval\s*\(', r'exec\s*\(', r'open\s*\(',
                r'os\.', r'subprocess\.', r'import\s+os', r'import\s+subprocess',
                r'import\s+sys', r'sys\.', r'__builtins__', r'globals\(\)',
                r'locals\(\)', r'rm\s+-', r'del\s+', r'format\s*\(.*\)\.__globals__'
            ]

            for pattern in dangerous_patterns:
                if re.search(pattern, code, re.IGNORECASE):
                    return "é”™è¯¯ï¼šæ£€æµ‹åˆ°å¯èƒ½ä¸å®‰å…¨çš„ä»£ç ï¼Œæ— æ³•æ‰§è¡Œ"

            # é¢å¤–çš„å®‰å…¨æ£€æŸ¥
            if any(keyword in code for keyword in ['__', 'breakpoint', 'compile', 'memoryview']):
                return "é”™è¯¯ï¼šä»£ç åŒ…å«å—é™å…³é”®å­—"

            cleaned_code = self._clean_python_code(code)
            if not cleaned_code:
                return "é”™è¯¯ï¼šä»£ç ä¸ºç©ºæˆ–æ— æ³•å¤„ç†"

            # ä½¿ç”¨subprocessæ‰§è¡Œä»£ç 
            result = subprocess.run(
                [sys.executable, "-c", cleaned_code],
                capture_output=True,
                text=True,
                timeout=10,
                cwd=os.path.dirname(os.path.abspath(__file__)) if __file__ else os.getcwd()
            )

            if result.returncode == 0:
                output = result.stdout.strip()
                return f"âœ… æ‰§è¡ŒæˆåŠŸ:\n{output}" if output else "âœ… ä»£ç æ‰§è¡ŒæˆåŠŸï¼ˆæ— è¾“å‡ºï¼‰"
            else:
                error_msg = result.stderr.strip()
                return f"âŒ æ‰§è¡Œé”™è¯¯:\n{error_msg}"

        except subprocess.TimeoutExpired:
            return "â° é”™è¯¯ï¼šä»£ç æ‰§è¡Œè¶…æ—¶ï¼Œå¯èƒ½å­˜åœ¨æ— é™å¾ªç¯"
        except Exception as e:
            return f"âš ï¸ æ‰§è¡Œå¼‚å¸¸: {str(e)}"

    def syntax_checker(self, code: str) -> str:
        """æ£€æŸ¥Pythonä»£ç çš„è¯­æ³•æ­£ç¡®æ€§ã€‚"""
        cleaned_code = _extract_code_snippet(code)
        if not cleaned_code:
            return "âŒ é”™è¯¯ï¼šä»£ç ä¸ºç©º"

        lines = cleaned_code.splitlines()
        try:
            tree = ast.parse(cleaned_code)
        except SyntaxError as e:
            context = _format_error_context(lines, e.lineno, e.offset)
            error_details = [
                "âŒ è¯­æ³•é”™è¯¯",
                f"- ç±»å‹: {e.msg}",
                f"- ä½ç½®: ç¬¬{e.lineno}è¡Œ, ç¬¬{e.offset}åˆ—" if e.lineno and e.offset else "",
            ]
            if context:
                error_details.append("\nä»£ç ä¸Šä¸‹æ–‡ï¼š")
                error_details.append(context)
            error_details.append("\nå»ºè®®ï¼šæ£€æŸ¥ç¼©è¿›ã€é—æ¼çš„å†’å·ã€æ‹¬å·æˆ–å¼•å·æ˜¯å¦åŒ¹é…ã€‚")
            return "\n".join([line for line in error_details if line])
        except Exception as e:
            return f"âš ï¸ è¯­æ³•æ£€æŸ¥å¼‚å¸¸: {str(e)}"

        structure = _collect_code_structure(tree)
        docstring_issues = _docstring_gaps(tree)
        style_notes = _basic_style_checks(lines)
        name_issues = _detect_name_issues(tree)
        type_total = structure["type_hint_total"] or 1  # é¿å…é™¤0
        type_ratio = structure["type_hint_annotated"] / type_total * 100

        report = [
            "âœ… è¯­æ³•æ£€æŸ¥é€šè¿‡ï¼šæœªå‘ç°è¯­æ³•é”™è¯¯",
            "",
            "ç»“æ„æ¦‚è§ˆ:",
            f"â€¢ æ€»è¡Œæ•°: {len(lines)}",
            f"â€¢ å‡½æ•°æ•°é‡: {len(structure['functions']) + len(structure['async_functions'])}",
            f"â€¢ ç±»æ•°é‡: {len(structure['classes'])}",
            f"â€¢ å¼•å…¥æ¨¡å—: {', '.join(structure['imports']) or 'ï¼ˆæ— ï¼‰'}",
        ]

        if structure["async_functions"]:
            async_names = ", ".join(fn["name"] for fn in structure["async_functions"])
            report.append(f"- å¼‚æ­¥å‡½æ•°: {async_names}")

        report.extend([
            "",
            "ç±»å‹æ³¨è§£è¦†ç›–ç‡:",
            f"â€¢ å‚æ•°æ³¨è§£: {type_ratio:.0f}% ({structure['type_hint_annotated']}/{structure['type_hint_total']})"
            if structure["type_hint_total"]
            else "â€¢ æœªæ£€æµ‹åˆ°å¯ç»Ÿè®¡çš„å‡½æ•°å‚æ•°"
        ])

        if docstring_issues:
            report.extend([
                "",
                "æ–‡æ¡£æç¤º:",
            ])
            for issue in docstring_issues[:5]:
                report.append(f"â€¢ {issue}")
            if len(docstring_issues) > 5:
                report.append("â€¢ ...")

        if name_issues:
            report.extend([
                "",
                "å¯èƒ½çš„åç§°æˆ–ä½œç”¨åŸŸé—®é¢˜:",
            ])
            suggestions = {
                "printf": "æ£€æµ‹åˆ° printfï¼ŒPython ä¸­è¯·ä½¿ç”¨ print()ã€‚",
                "scanf": "æ£€æµ‹åˆ° scanfï¼ŒPython ä¸­å¯ä½¿ç”¨ input()ã€‚",
                "system": "è¯·ç¡®è®¤æ˜¯å¦éœ€è¦ import os åä½¿ç”¨ os.systemã€‚",
            }
            for name in sorted(name_issues.keys())[:6]:
                lines_info = ", ".join(f"ç¬¬{ln}è¡Œ" for ln in name_issues[name][:5])
                extra = suggestions.get(name.lower(), "")
                extra_text = f" {extra}" if extra else ""
                report.append(f"â€¢ {name}: {lines_info}{extra_text}")
            if len(name_issues) > 6:
                report.append("â€¢ ...")

        if style_notes:
            report.extend([
                "",
                "é£æ ¼å»ºè®®ï¼ˆèŠ‚é€‰ï¼‰:",
            ])
            for note in style_notes[:5]:
                report.append(f"â€¢ {note}")
            if len(style_notes) > 5:
                report.append("â€¢ æ›´å¤šé£æ ¼é—®é¢˜è¯·æŸ¥çœ‹å®Œæ•´åˆ†æç»“æœ")

        return "\n".join(report)

    def python_documentation(self, topic: str) -> str:
        """æä¾›Pythonå®˜æ–¹æ–‡æ¡£ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚"""
        docs = {
            "åˆ—è¡¨æ¨å¯¼å¼": {
                "description": "åˆ—è¡¨æ¨å¯¼å¼æä¾›äº†åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼",
                "syntax": "[expression for item in iterable if condition]",
                "examples": [
                    "squares = [x**2 for x in range(5)]  # [0, 1, 4, 9, 16]",
                    "even_squares = [x**2 for x in range(10) if x % 2 == 0]  # [0, 4, 16, 36, 64]",
                    "pairs = [(x, y) for x in range(3) for y in range(3)]  # åµŒå¥—å¾ªç¯"
                ]
            },
            "è£…é¥°å™¨": {
                "description": "è£…é¥°å™¨ç”¨äºä¿®æ”¹å‡½æ•°/ç±»çš„è¡Œä¸º",
                "syntax": "@decorator",
                "examples": [
                    "import time\ndef timer_decorator(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        print(f'è¿è¡Œæ—¶é—´: {time.time()-start:.2f}ç§’')\n        return result\n    return wrapper\n\n@timer_decorator\ndef slow_func():\n    time.sleep(1)\n    return 'å®Œæˆ'"
                ]
            },
            "ç”Ÿæˆå™¨": {
                "description": "ç”Ÿæˆå™¨æ˜¯èŠ‚çœå†…å­˜çš„è¿­ä»£å™¨ï¼Œä½¿ç”¨yieldå…³é”®å­—",
                "syntax": "def generator_func(): yield value",
                "examples": [
                    "def number_gen(n):\n    for i in range(n):\n        yield i",
                    "squares = (x**2 for x in range(5))  # ç”Ÿæˆå™¨è¡¨è¾¾å¼"
                ]
            },
            "ä¸Šä¸‹æ–‡ç®¡ç†å™¨": {
                "description": "ç”¨äºç®¡ç†èµ„æºï¼ˆæ–‡ä»¶/è¿æ¥ï¼‰ï¼Œä½¿ç”¨withè¯­å¥",
                "syntax": "with context_manager as var:",
                "examples": [
                    "with open('file.txt', 'w') as f:\n    f.write('Hello')",
                    "class Timer:\n    def __enter__(self):\n        self.start = time.time()\n        return self\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        print(f'è€—æ—¶: {time.time()-self.start:.2f}ç§’')"
                ]
            },
            "å¼‚å¸¸å¤„ç†": {
                "description": "æ•è·è¿è¡Œæ—¶é”™è¯¯",
                "syntax": "try-except-else-finally",
                "examples": [
                    "try:\n    result = 10 / 2\nexcept ZeroDivisionError:\n    print('ä¸èƒ½é™¤ä»¥é›¶')\nelse:\n    print(f'ç»“æœ: {result}')\nfinally:\n    print('æ¸…ç†å®Œæˆ')"
                ]
            },
            "é¢å‘å¯¹è±¡": {
                "description": "æ”¯æŒç±»/å¯¹è±¡/ç»§æ‰¿",
                "syntax": "class ClassName:",
                "examples": [
                    "class Person:\n    species = 'äººç±»'\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n    def introduce(self):\n        return f'æˆ‘å«{self.name}ï¼Œ{self.age}å²'\n    @classmethod\n    def get_species(cls):\n        return cls.species"
                ]
            }
        }

        topic_lower = topic.lower().strip()
        for key in docs:
            if topic_lower in key.lower():
                doc = docs[key]
                response = f"## {key}\n\n**æè¿°**: {doc['description']}\n\n**è¯­æ³•**: `{doc['syntax']}`\n\n**ç¤ºä¾‹**:\n"
                for example in doc['examples']:
                    response += f"```python\n{example}\n```\n"
                return response

        # å¦‚æœåœ¨é¢„å®šä¹‰æ–‡æ¡£ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»æ‰‹å†Œä¸­æœç´¢
        handbook_result = self.handbook_search(topic)
        if "æœªæ‰¾åˆ°" not in handbook_result:
            return handbook_result

        available_topics = "\n".join([f"- {t}" for t in docs.keys()])
        return f"æœªæ‰¾åˆ°'{topic}'çš„æ–‡æ¡£ã€‚å¯ç”¨ä¸»é¢˜ï¼š\n{available_topics}"

    def code_analyzer(self, code: str) -> str:
        """åˆ†æPythonä»£ç ï¼Œæä¾›æ”¹è¿›å»ºè®®å’Œæœ€ä½³å®è·µã€‚"""
        cleaned_code = _extract_code_snippet(code)
        if not cleaned_code:
            return "âŒ é”™è¯¯ï¼šä»£ç ä¸ºç©º"

        lines = cleaned_code.splitlines()
        try:
            tree = ast.parse(cleaned_code)
        except SyntaxError as e:
            context = _format_error_context(lines, e.lineno, e.offset)
            details = [
                "âŒ æ£€æµ‹åˆ°è¯­æ³•é”™è¯¯ï¼Œæ— æ³•ç»§ç»­åˆ†æã€‚",
                f"- é”™è¯¯: {e.msg}",
                f"- ä½ç½®: ç¬¬{e.lineno}è¡Œ, ç¬¬{e.offset}åˆ—" if e.lineno and e.offset else "",
            ]
            if context:
                details.append("\nä»£ç ä¸Šä¸‹æ–‡ï¼š")
                details.append(context)
            details.append("\nè¯·å…ˆä¿®å¤è¯­æ³•é—®é¢˜åå†æ¬¡è¿è¡Œåˆ†æã€‚")
            return "\n".join([line for line in details if line])

        structure = _collect_code_structure(tree)
        docstring_issues = _docstring_gaps(tree)
        style_notes = _basic_style_checks(lines)

        snake_case = re.compile(r'^[a-z_][a-z0-9_]*$')
        pascal_case = re.compile(r'^[A-Z][A-Za-z0-9]+$')

        complexity_notes = []
        maintainability_notes = []
        risk_notes = []
        naming_notes = []

        signature_lookup = {}
        for fn in structure["functions"] + structure["async_functions"]:
            signature_lookup[(fn["name"], fn["lineno"])] = fn

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                signature_meta = signature_lookup.get((node.name, node.lineno))
                args_repr = ", ".join(signature_meta["args"]) if signature_meta else "..."
                signature = f"{'async ' if isinstance(node, ast.AsyncFunctionDef) else ''}{node.name}({args_repr})"
                if signature_meta and signature_meta["returns"]:
                    signature += f" -> {signature_meta['returns']}"

                length = (getattr(node, 'end_lineno', node.lineno) or node.lineno) - node.lineno + 1
                complexity = _cyclomatic_complexity(node)
                depth = _max_nesting_depth(node)

                if complexity >= 12:
                    complexity_notes.append(f"- `{signature}` çš„åœˆå¤æ‚åº¦ä¸º {complexity}ï¼Œå»ºè®®æ‹†è§£é€»è¾‘æˆ–æå–å­å‡½æ•°")
                elif complexity >= 8:
                    complexity_notes.append(f"- `{signature}` çš„åœˆå¤æ‚åº¦ä¸º {complexity}ï¼Œæ¥è¿‘ä¸Šé™ï¼Œæ³¨æ„æ§åˆ¶æ¡ä»¶åˆ†æ”¯")

                if length > 80:
                    maintainability_notes.append(f"- `{signature}` é•¿åº¦ä¸º {length} è¡Œï¼Œå»ºè®®æ‹†åˆ†ä¸ºæ›´å°çš„å‡½æ•°")
                elif length > 40:
                    maintainability_notes.append(f"- `{signature}` é•¿åº¦ä¸º {length} è¡Œï¼Œå¯è€ƒè™‘æå–å…¬å…±é€»è¾‘")

                if depth > 4:
                    maintainability_notes.append(f"- `{signature}` çš„åµŒå¥—æ·±åº¦è¾¾åˆ° {depth} å±‚ï¼Œå»ºè®®é™ä½åµŒå¥—")

                # å¯å˜é»˜è®¤å‚æ•°
                mutable_nodes = (ast.List, ast.Dict, ast.Set)
                defaults = list(node.args.defaults) + [d for d in node.args.kw_defaults if d]
                if any(isinstance(default, mutable_nodes) for default in defaults):
                    risk_notes.append(f"- `{signature}` ä½¿ç”¨å¯å˜å¯¹è±¡ä½œä¸ºé»˜è®¤å‚æ•°ï¼Œå¯èƒ½å¼•å‘å…±äº«çŠ¶æ€é—®é¢˜")

                # å‘½åè§„èŒƒ
                if not snake_case.match(node.name):
                    naming_notes.append(f"- å‡½æ•° `{node.name}` å»ºè®®ä½¿ç”¨ snake_case å‘½å")

            elif isinstance(node, ast.ClassDef):
                if not pascal_case.match(node.name):
                    naming_notes.append(f"- ç±» `{node.name}` å»ºè®®ä½¿ç”¨å¸•æ–¯å¡å‘½åï¼ˆå¦‚ `MyClass`ï¼‰")

            elif isinstance(node, ast.ExceptHandler):
                if node.type is None:
                    risk_notes.append("- æ£€æµ‹åˆ°è£¸ `except:`ï¼Œè¯·æ•è·å…·ä½“å¼‚å¸¸ç±»å‹")
                elif isinstance(node.type, ast.Name) and node.type.id in ("Exception", "BaseException"):
                    risk_notes.append("- æ•è·äº†è¿‡äºå®½æ³›çš„å¼‚å¸¸ `Exception/BaseException`ï¼Œå»ºè®®æ›´ç²¾ç¡®")

            elif isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name) and node.func.id in {"eval", "exec"}:
                    risk_notes.append(f"- ä½¿ç”¨ `{node.func.id}` å¯èƒ½å¸¦æ¥å®‰å…¨é£é™©")
                elif isinstance(node.func, ast.Attribute):
                    attr = node.func.attr
                    owner = getattr(node.func.value, 'id', None)
                    if owner == 'os' and attr in {'system', 'popen'}:
                        risk_notes.append(f"- è°ƒç”¨ `os.{attr}` å¯èƒ½æ‰§è¡Œç³»ç»Ÿå‘½ä»¤ï¼Œè¯·ç¡®è®¤å®‰å…¨æ€§")
                    if owner == 'subprocess' and attr in {'Popen', 'call', 'run'}:
                        risk_notes.append("- ä½¿ç”¨ `subprocess` æ‰§è¡Œå¤–éƒ¨å‘½ä»¤ï¼Œè¯·ç¡®ä¿å‚æ•°å®‰å…¨")

            elif isinstance(node, ast.Global):
                risk_notes.append(f"- åœ¨ç¬¬{node.lineno}è¡Œä½¿ç”¨ `global` ï¼Œå»ºè®®é€šè¿‡å‚æ•°æˆ–è¿”å›å€¼ä¼ é€’æ•°æ®")

        todo_lines = [idx for idx, line in enumerate(lines, 1) if 'TODO' in line or 'FIXME' in line]
        if todo_lines:
            maintainability_notes.append(f"- æ£€æµ‹åˆ°æœªå®Œæˆæ ‡è®°ï¼ˆTODO/FIXMEï¼‰ä½äºè¡Œ: {', '.join(map(str, todo_lines[:5]))}")

        pattern_checks = [
            (r'from\s+\w+\s+import\s+\*', "- é¿å… `from module import *`ï¼Œå¯èƒ½æ±¡æŸ“å‘½åç©ºé—´"),
            (r'while\s+True\s*:', "- `while True` è¯·ç¡®ä¿å­˜åœ¨é€€å‡ºæ¡ä»¶"),
        ]
        for pattern, message in pattern_checks:
            if re.search(pattern, cleaned_code):
                risk_notes.append(message)

        report = ["ä»£ç åˆ†æç»“æœï¼ˆä¸“ä¸šå¢å¼ºç‰ˆï¼‰"]
        report.extend([
            "",
            "ç»“æ„æ¦‚è§ˆ:",
            f"â€¢ è¡Œæ•°: {len(lines)}",
            f"â€¢ å‡½æ•°/å¼‚æ­¥å‡½æ•°: {len(structure['functions']) + len(structure['async_functions'])}",
            f"â€¢ ç±»: {len(structure['classes'])}",
            f"â€¢ å¯¼å…¥: {', '.join(structure['imports']) or 'ï¼ˆæ— ï¼‰'}",
        ])

        if structure["type_hint_total"]:
            ratio = structure["type_hint_annotated"] / structure["type_hint_total"] * 100
            report.append(
                f"â€¢ å‚æ•°ç±»å‹æ³¨è§£è¦†ç›–ç‡: {ratio:.0f}% ({structure['type_hint_annotated']}/{structure['type_hint_total']})")

        if complexity_notes:
            report.extend([
                "",
                "å¤æ‚åº¦ä¸å¯ç»´æŠ¤æ€§:",
                *complexity_notes[:6]
            ])
        if maintainability_notes:
            if "å¤æ‚åº¦ä¸å¯ç»´æŠ¤æ€§:" not in report:
                report.extend(["", "å¤æ‚åº¦ä¸å¯ç»´æŠ¤æ€§:"])
            for note in maintainability_notes[:6]:
                report.append(note)

        if docstring_issues:
            report.extend([
                "",
                "æ–‡æ¡£ä¸å¯è¯»æ€§:",
            ])
            for issue in docstring_issues[:6]:
                report.append(f"â€¢ {issue}")

        if naming_notes:
            report.extend([
                "",
                "å‘½åè§„èŒƒ:",
            ])
            for note in naming_notes[:5]:
                report.append(f"â€¢ {note}")

        if style_notes:
            report.extend([
                "",
                "é£æ ¼å»ºè®®ï¼ˆèŠ‚é€‰ï¼‰:",
            ])
            for note in style_notes[:6]:
                report.append(f"â€¢ {note}")

        if risk_notes:
            report.extend([
                "",
                "æ½œåœ¨é£é™©:",
            ])
            for note in risk_notes[:6]:
                report.append(f"â€¢ {note}")

        if not any([complexity_notes, maintainability_notes, docstring_issues, naming_notes, style_notes, risk_notes]):
            report.extend([
                "",
                "âœ… æœªå‘ç°æ˜æ˜¾çš„é£æ ¼æˆ–è´¨é‡é—®é¢˜ï¼Œä»£ç æ•´ä½“è‰¯å¥½ã€‚"
            ])

        return "\n".join(report)

    def _clean_python_code(self, code: str) -> str:
        """æ¸…ç†è¦æ‰§è¡Œçš„Pythonä»£ç """
        code = re.sub(r'```python\s*|\s*```', '', code).strip()
        if not code:
            return ""

        lines = code.split('\n')
        last_line = lines[-1].strip()

        # å¯¹å­¤ç«‹è¡¨è¾¾å¼æ·»åŠ printï¼ˆä»…åœ¨å®‰å…¨çš„æƒ…å†µä¸‹ï¼‰
        if (last_line and not last_line.startswith(
                (' ', '\t', 'def ', 'class ', 'import ', 'from ', 'if ', 'for ', 'while ', 'with ', '#', 'print(',
                 'return', 'yield', 'raise', 'try', 'except', 'finally'))
                and '=' not in last_line
                and not last_line.endswith((':', ';'))
                and not any(keyword in last_line for keyword in ['lambda', 'async', 'await'])):
            lines[-1] = f"print({last_line})"

        return '\n'.join(lines)

    def _detect_tool_usage(self, question: str) -> dict:
        """æ£€æµ‹ç”¨æˆ·é—®é¢˜æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·"""
        question_lower = question.lower()

        tool_usage = {
            "use_tool": False,
            "tool_name": None,
            "tool_input": None
        }

        # æ£€æµ‹åŸºç¡€æ¦‚å¿µé—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨æ‰‹å†Œæœç´¢
        basic_concepts = ['æ˜¯ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'æ¦‚å¿µ', 'ä»‹ç»', 'è®²è§£', 'è¯´æ˜', 'å«ä¹‰']
        if any(concept in question_lower for concept in basic_concepts):
            # æå–å…³é”®è¯
            words = question_lower.split()
            keywords = [word for word in words if len(word) > 2 and word not in ['python', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€æ ·']]
            if keywords:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "handbook_search",
                    "tool_input": ' '.join(keywords[:3])  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
                })

        # æ£€æµ‹ä»£ç æ‰§è¡Œ
        elif any(word in question_lower for word in ['æ‰§è¡Œ', 'è¿è¡Œ', 'è¿è¡Œä»£ç ', 'æ‰§è¡Œä»£ç ', 'test', 'run']):
            code_match = re.search(r'```python\s*(.*?)\s*```', question, re.DOTALL)
            if code_match:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "code_executor",
                    "tool_input": code_match.group(1)
                })

        # æ£€æµ‹è¯­æ³•æ£€æŸ¥
        elif any(word in question_lower for word in ['è¯­æ³•', 'è¯­æ³•æ£€æŸ¥', 'è¯­æ³•é”™è¯¯', 'syntax']):
            code_match = re.search(r'```python\s*(.*?)\s*```', question, re.DOTALL)
            if code_match:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "syntax_checker",
                    "tool_input": code_match.group(1)
                })

        # æ£€æµ‹ä»£ç åˆ†æ
        elif any(word in question_lower for word in ['åˆ†æ', 'ä¼˜åŒ–', 'æ”¹è¿›', 'ä»£ç åˆ†æ', 'analyze']):
            code_match = re.search(r'```python\s*(.*?)\s*```', question, re.DOTALL)
            if code_match:
                tool_usage.update({
                    "use_tool": True,
                    "tool_name": "code_analyzer",
                    "tool_input": code_match.group(1)
                })

        # æ£€æµ‹æ–‡æ¡£æŸ¥è¯¢
        elif any(word in question_lower for word in ['æ–‡æ¡£', 'è¯´æ˜', 'ä»‹ç»', 'ä»€ä¹ˆæ˜¯', 'documentation']):
            for topic in ['åˆ—è¡¨æ¨å¯¼å¼', 'è£…é¥°å™¨', 'ç”Ÿæˆå™¨', 'ä¸Šä¸‹æ–‡ç®¡ç†å™¨', 'å¼‚å¸¸å¤„ç†', 'é¢å‘å¯¹è±¡', 'å¼‚æ­¥ç¼–ç¨‹',
                          'ç±»å‹æ³¨è§£']:
                if topic in question:
                    tool_usage.update({
                        "use_tool": True,
                        "tool_name": "python_documentation",
                        "tool_input": topic
                    })
                    break

        return tool_usage

    def ask_question(self, question: str) -> str:
        """å‘æ™ºèƒ½ä½“æé—®å…³äºPythonç¼–ç¨‹çš„é—®é¢˜"""
        try:
            # æ£€æµ‹æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
            tool_info = self._detect_tool_usage(question)

            if tool_info["use_tool"]:
                tool_name = tool_info["tool_name"]
                tool_input = tool_info["tool_input"]

                if tool_name in self.tools:
                    print(f"ğŸ”§ ä½¿ç”¨å·¥å…·: {tool_name}")
                    tool_result = self.tools[tool_name](tool_input)

                    # å¦‚æœæœ‰LLMï¼Œè®©LLMæ¥è§£é‡Šå·¥å…·ç»“æœ
                    if self.llm:
                        enhanced_prompt = f"""ç”¨æˆ·çš„é—®é¢˜: {question}

å·¥å…·æ‰§è¡Œç»“æœ:
{tool_result}

è¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœï¼Œç»™ç”¨æˆ·ä¸€ä¸ªå®Œæ•´ã€ä¸“ä¸šçš„å›ç­”ã€‚ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨Markdownæ ¼å¼ã€‚"""

                        messages = [
                            SystemMessage(
                                content="ä½ æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹åŠ©æ‰‹ï¼Œè¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœç»™ç”¨æˆ·æä¾›ä¸“ä¸šã€å®Œæ•´çš„å›ç­”ã€‚"),
                            HumanMessage(content=enhanced_prompt)
                        ]
                        response = self.llm.invoke(messages)
                        return response.content
                    else:
                        # æ— LLMæ—¶ç›´æ¥è¿”å›å·¥å…·ç»“æœ
                        return f"**å·¥å…·æ‰§è¡Œç»“æœ**:\n\n{tool_result}"

            # å¦‚æœæ²¡æœ‰ä½¿ç”¨å·¥å…·æˆ–è€…å·¥å…·ä½¿ç”¨å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨LLM
            if self.llm:
                # å…ˆå°è¯•ä»æ‰‹å†Œä¸­æœç´¢ç›¸å…³ä¿¡æ¯
                handbook_info = ""
                try:
                    # æå–å…³é”®è¯è¿›è¡Œæ‰‹å†Œæœç´¢
                    words = question.lower().split()
                    keywords = [word for word in words if
                                len(word) > 2 and word not in ['python', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€æ ·']]
                    if keywords:
                        search_query = ' '.join(keywords[:2])
                        handbook_results = self.handbook.search_content(search_query)
                        if handbook_results:
                            handbook_info = "\n\n## ğŸ“š æ‰‹å†Œå‚è€ƒä¿¡æ¯\n\n"
                            for result in handbook_results[:2]:  # æœ€å¤š2ä¸ªç»“æœ
                                handbook_info += f"**{result['section']}**\n{result['content'][:200]}...\n\n"
                except Exception as e:
                    logger.warning(f"æ‰‹å†Œæœç´¢å¤±è´¥: {e}")

                enhanced_question = question
                if handbook_info:
                    enhanced_question += f"\n\nä»¥ä¸‹æ˜¯æ¥è‡ªPythonèƒŒè®°æ‰‹å†Œçš„ç›¸å…³ä¿¡æ¯ï¼Œè¯·å‚è€ƒï¼š{handbook_info}"

                messages = [
                    SystemMessage(content=self.system_prompt),
                    HumanMessage(content=enhanced_question)
                ]
                response = self.llm.invoke(messages)
                return response.content
            else:
                return self._local_answer(question)

        except Exception as e:
            error_msg = f"æé—®é”™è¯¯: {str(e)}"
            print(f"Error: {error_msg}")
            return f"âš ï¸ {error_msg}\nè¯·æ£€æŸ¥APIå¯†é’¥æˆ–ç½‘ç»œè¿æ¥"

    def _local_answer(self, question: str) -> str:
        """æ— APIæ—¶çš„æœ¬åœ°å›ç­”"""
        q = question.lower()

        # å°è¯•ä½¿ç”¨å·¥å…·
        tool_info = self._detect_tool_usage(question)
        if tool_info["use_tool"] and tool_info["tool_name"] in self.tools:
            return self.tools[tool_info["tool_name"]](tool_info["tool_input"])

        # å°è¯•ä»æ‰‹å†Œä¸­æœç´¢
        try:
            words = q.split()
            keywords = [word for word in words if len(word) > 2]
            if keywords:
                handbook_result = self.handbook_search(' '.join(keywords[:2]))
                if "æœªæ‰¾åˆ°" not in handbook_result:
                    return handbook_result
        except:
            pass

        # æœ¬åœ°çŸ¥è¯†åº“
        if any(w in q for w in ['åˆ—è¡¨æ¨å¯¼', 'list comprehension']):
            return self.python_documentation("åˆ—è¡¨æ¨å¯¼å¼")
        elif any(w in q for w in ['è£…é¥°å™¨', 'decorator']):
            return self.python_documentation("è£…é¥°å™¨")
        elif any(w in q for w in ['ç”Ÿæˆå™¨', 'generator']):
            return self.python_documentation("ç”Ÿæˆå™¨")
        elif any(w in q for w in ['ä¸Šä¸‹æ–‡ç®¡ç†', 'context manager', 'with']):
            return self.python_documentation("ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
        elif any(w in q for w in ['å¼‚å¸¸', 'exception', 'try', 'except']):
            return self.python_documentation("å¼‚å¸¸å¤„ç†")
        elif any(w in q for w in ['ç±»', 'class', 'å¯¹è±¡', 'object', 'é¢å‘å¯¹è±¡']):
            return self.python_documentation("é¢å‘å¯¹è±¡")
        elif any(w in q for w in ['è¯­æ³•', 'syntax', 'æ£€æŸ¥']):
            return "è¯·æä¾›Pythonä»£ç ï¼Œæˆ‘å°†æ£€æŸ¥è¯­æ³•æ­£ç¡®æ€§ï¼ˆç¤ºä¾‹ï¼š\n```python\ndef add(a,b): return a+b\n```ï¼‰"
        elif any(w in q for w in ['æ‰§è¡Œ', 'è¿è¡Œ', 'test', 'run']):
            return "è¯·æä¾›Pythonä»£ç ï¼Œæˆ‘å°†æ‰§è¡Œå¹¶è¿”å›ç»“æœï¼ˆç¤ºä¾‹ï¼š\n```python\nprint([i for i in range(5)])\n```ï¼‰"
        elif any(w in q for w in ['åˆ†æ', 'ä¼˜åŒ–', 'æ”¹è¿›']):
            return "è¯·æä¾›Pythonä»£ç ï¼Œæˆ‘å°†åˆ†æå¹¶ç»™å‡ºæ”¹è¿›å»ºè®®ï¼ˆç¤ºä¾‹ï¼š\n```python\ndef calc():\n    total=0\n    for i in range(10): total+=i\n    print(total)\n```ï¼‰"
        else:
            return "âš ï¸ æœ¬åœ°æ¨¡å¼ä»…æ”¯æŒä»¥ä¸‹ä¸»é¢˜ï¼š\n- åˆ—è¡¨æ¨å¯¼å¼ã€è£…é¥°å™¨ã€ç”Ÿæˆå™¨\n- ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€å¼‚å¸¸å¤„ç†ã€é¢å‘å¯¹è±¡\n- ä»£ç è¯­æ³•æ£€æŸ¥ã€æ‰§è¡Œã€åˆ†æä¼˜åŒ–\nå¦‚éœ€æ›´å¤šå›ç­”ï¼Œè¯·é…ç½®DeepSeek/OpenAI APIå¯†é’¥"